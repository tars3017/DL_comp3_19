{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.2.3) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import re\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5427 vocabularies in total\n",
      "Word to id mapping, for example: flower -> 1\n",
      "Id to word mapping, for example: 1 -> flower\n",
      "Tokens: <PAD>: 5427; <RARE>: 5428\n"
     ]
    }
   ],
   "source": [
    "dictionary_path = './dictionary'\n",
    "vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s' % ('flower', word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s' % ('1', id2word_dict['1']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the flower shown has yellow anther red pistil and bright red petals.\n",
      "['9', '1', '82', '5', '11', '70', '20', '31', '3', '29', '20', '2', '5427', '5427', '5427', '5427', '5427', '5427', '5427', '5427']\n"
     ]
    }
   ],
   "source": [
    "def sent2IdList(line, MAX_SEQ_LENGTH=20):\n",
    "    MAX_SEQ_LIMIT = MAX_SEQ_LENGTH\n",
    "    padding = 0\n",
    "    \n",
    "    # data preprocessing, remove all puntuation in the texts\n",
    "    prep_line = re.sub('[%s]' % re.escape(string.punctuation), ' ', line.rstrip())\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('  ', ' ')\n",
    "    prep_line = prep_line.replace('.', '')\n",
    "    tokens = prep_line.split(' ')\n",
    "    tokens = [\n",
    "        tokens[i] for i in range(len(tokens))\n",
    "        if tokens[i] != ' ' and tokens[i] != ''\n",
    "    ]\n",
    "    l = len(tokens)\n",
    "    padding = MAX_SEQ_LIMIT - l\n",
    "    \n",
    "    # make sure length of each text is equal to MAX_SEQ_LENGTH, and replace the less common word with <RARE> token\n",
    "    for i in range(padding):\n",
    "        tokens.append('<PAD>')\n",
    "    line = [\n",
    "        word2Id_dict[tokens[k]]\n",
    "        if tokens[k] in word2Id_dict else word2Id_dict['<RARE>']\n",
    "        for k in range(len(tokens))\n",
    "    ]\n",
    "\n",
    "    return line\n",
    "\n",
    "text = \"the flower shown has yellow anther red pistil and bright red petals.\"\n",
    "print(text)\n",
    "print(sent2IdList(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7370 image in training data\n"
     ]
    }
   ],
   "source": [
    "data_path = './dataset'\n",
    "df = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6734</th>\n",
       "      <td>[[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...</td>\n",
       "      <td>./102flowers/image_06734.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6736</th>\n",
       "      <td>[[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...</td>\n",
       "      <td>./102flowers/image_06736.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6737</th>\n",
       "      <td>[[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...</td>\n",
       "      <td>./102flowers/image_06737.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6738</th>\n",
       "      <td>[[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...</td>\n",
       "      <td>./102flowers/image_06738.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6739</th>\n",
       "      <td>[[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...</td>\n",
       "      <td>./102flowers/image_06739.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Captions  \\\n",
       "ID                                                        \n",
       "6734  [[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...   \n",
       "6736  [[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...   \n",
       "6737  [[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...   \n",
       "6738  [[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...   \n",
       "6739  [[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...   \n",
       "\n",
       "                         ImagePath  \n",
       "ID                                  \n",
       "6734  ./102flowers/image_06734.jpg  \n",
       "6736  ./102flowers/image_06736.jpg  \n",
       "6737  ./102flowers/image_06737.jpg  \n",
       "6738  ./102flowers/image_06738.jpg  \n",
       "6739  ./102flowers/image_06739.jpg  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this competition, you have to generate image in size 64x64x3\n",
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_CHANNEL = 3\n",
    "\n",
    "def training_data_generator(caption, image_path):\n",
    "    # load in the image according to image path\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img.set_shape([None, None, 3])\n",
    "    img = tf.image.resize(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    img.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "    caption = tf.cast(caption, tf.int32)\n",
    "\n",
    "    return img, caption\n",
    "\n",
    "def dataset_generator(filenames, batch_size, data_generator):\n",
    "    # load the training data into two NumPy arrays\n",
    "    df = pd.read_pickle(filenames)\n",
    "    captions = df['Captions'].values\n",
    "    caption = []\n",
    "    # each image has 1 to 10 corresponding captions\n",
    "    # we choose one of them randomly for training\n",
    "    for i in range(len(captions)):\n",
    "        caption.append(random.choice(captions[i]))\n",
    "    caption = np.asarray(caption)\n",
    "    caption = caption.astype(np.int)\n",
    "    image_path = df['ImagePath'].values\n",
    "    \n",
    "    # assume that each row of `features` corresponds to the same row as `labels`.\n",
    "    assert caption.shape[0] == image_path.shape[0]\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, image_path))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(len(caption)).batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-c5136386d970>:28: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  caption = caption.astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "dataset = dataset_generator(data_path + '/text2ImgData.pkl', BATCH_SIZE, training_data_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Encode text (a caption) into hidden representation\n",
    "    input: text, which is a list of ids\n",
    "    output: embedding, or hidden representation of input text in dimension of RNN_HIDDEN_SIZE\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.batch_size = self.hparas['BATCH_SIZE']\n",
    "        \n",
    "        # embedding with tensorflow API\n",
    "        self.embedding = layers.Embedding(self.hparas['VOCAB_SIZE'], self.hparas['EMBED_DIM'])\n",
    "        # RNN, here we use GRU cell, another common RNN cell similar to LSTM\n",
    "        self.gru = layers.GRU(self.hparas['RNN_HIDDEN_SIZE'],\n",
    "                              return_sequences=True,\n",
    "                              return_state=True,\n",
    "                              recurrent_initializer='glorot_uniform')\n",
    "    \n",
    "    def call(self, text, hidden):\n",
    "        text = self.embedding(text)\n",
    "        # output: (batch_size, sequence_length, rnn_hidden_size)\n",
    "        output, state = self.gru(text, initial_state = hidden)\n",
    "        # we only need to output the last hidden state of RNN\n",
    "        return output[:, -1, :], state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.hparas['BATCH_SIZE'], self.hparas['RNN_HIDDEN_SIZE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TFBertModel, BertTokenizer\n",
    "\n",
    "# # Need to modify the input as raw text?\n",
    "# class TextEncoder(tf.keras.Model):\n",
    "#     def __init__(self, hparas):\n",
    "#         super(TextEncoder, self).__init__()\n",
    "#         self.hparas = hparas\n",
    "        \n",
    "#         # Load pre-trained BERT model and tokenizer\n",
    "#         self.embedding = tf.keras.layers.Embedding(\n",
    "#             input_dim = self.hparas['VOCAB_SIZE'],\n",
    "#             output_dim = 768\n",
    "#         )\n",
    "#         self.bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "#         self.bert_model.trainable = False\n",
    "        \n",
    "#         # Projection layer to match hidden size\n",
    "#         self.projection = tf.keras.layers.Dense(\n",
    "#             self.hparas.get('RNN_HIDDEN_SIZE', 768)\n",
    "#         )\n",
    "    \n",
    "#     def call(self, text, hidden=None):\n",
    "#         # Tokenize and encode text\n",
    "#         attention_mask = tf.cast(text != word2Id_dict['<PAD>'], tf.int32)\n",
    "#         embeddings = self.embedding(text)\n",
    "#         outputs = self.bert_model(\n",
    "#             input_ids=text,\n",
    "#             attention_mask=attention_mask\n",
    "#         )\n",
    "#         pooled = outputs.pooler_output\n",
    "#         final_embedding = self.projection(pooled)\n",
    "\n",
    "#         return final_embedding, final_embedding\n",
    "\n",
    "    \n",
    "#     def initialize_hidden_state(self):\n",
    "#         return tf.zeros((self.hparas['BATCH_SIZE'], self.hparas.get('RNN_HIDDEN_SIZE', 768)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Generate fake image based on given text(hidden representation) and noise z\n",
    "    input: text and noise\n",
    "    output: fake image with size 64*64*3\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(Generator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        # self.flatten = tf.keras.layers.Flatten()\n",
    "        # self.d1 = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n",
    "        # self.d2 = tf.keras.layers.Dense(64*64*3)\n",
    "\n",
    "        # Dense layer for text embedding\n",
    "        self.text_flatten = tf.keras.layers.Flatten()\n",
    "        self.text_dense1 = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n",
    "        self.text_dense2 = tf.keras.layers.Dense(64 * 8 * 4 * 4)\n",
    "\n",
    "        # Convolution layers for image generation\n",
    "        self.deconv1 = tf.keras.layers.Conv2DTranspose(64 * 4, (5, 5), strides=(2, 2), padding='same')\n",
    "        self.deconv2 = tf.keras.layers.Conv2DTranspose(64 * 2, (5, 5), strides=(2, 2), padding='same')\n",
    "        self.deconv3 = tf.keras.layers.Conv2DTranspose(64 * 1, (5, 5), strides=(2, 2), padding='same')\n",
    "        self.final_conv = tf.keras.layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same')\n",
    "\n",
    "        # Batch normalization\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        \n",
    "    def call(self, text, noise_z, training=True):\n",
    "        # text = self.flatten(text)\n",
    "        # text = self.d1(text)\n",
    "        # text = tf.nn.leaky_relu(text)\n",
    "        \n",
    "        # # concatenate input text and random noise\n",
    "        # text_concat = tf.concat([noise_z, text], axis=1)\n",
    "        # text_concat = self.d2(text_concat)\n",
    "        \n",
    "        # logits = tf.reshape(text_concat, [-1, 64, 64, 3])\n",
    "        # output = tf.nn.tanh(logits)\n",
    "\n",
    "        text = self.text_flatten(text)\n",
    "        text = self.text_dense1(text)\n",
    "        text = tf.nn.leaky_relu(text)\n",
    "\n",
    "        # concatenate input text and random noise\n",
    "        text_concat = tf.concat([noise_z, text], axis=1)\n",
    "        text_concat = self.text_dense2(text_concat)\n",
    "\n",
    "        # Reshape and generate image\n",
    "        x = tf.reshape(text_concat, [-1, 4, 4, 64 * 8])\n",
    "\n",
    "        # Deconvolution layers\n",
    "        x = self.deconv1(x)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.deconv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.deconv3(x)\n",
    "        x = self.bn3(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        # Final layer with tanh activation\n",
    "        logits = self.final_conv(x)\n",
    "        output = tf.nn.tanh(logits)\n",
    "        \n",
    "        return logits, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Differentiate the real and fake image\n",
    "    input: image and corresponding text\n",
    "    output: labels, the real image should be 1, while the fake should be 0\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        # self.flatten = tf.keras.layers.Flatten()\n",
    "        # self.d_text = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n",
    "        # self.d_img = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n",
    "        # self.d = tf.keras.layers.Dense(1)\n",
    "\n",
    "        # Convolution layers\n",
    "        self.conv1 = tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64 * 2, (5, 5), strides=(2, 2), padding='same')\n",
    "        self.conv3 = tf.keras.layers.Conv2D(64 * 4, (5, 5), strides=(2, 2), padding='same')\n",
    "\n",
    "        # Batch normalization\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        # Flatten and dense layers\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "        # Conditional processing\n",
    "        self.text_dense = tf.keras.layers.Dense(64 * 4)\n",
    "        self.final_dense = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, img, text, training=True):\n",
    "        # text = self.flatten(text)\n",
    "        # text = self.d_text(text)\n",
    "        # text = tf.nn.leaky_relu(text)\n",
    "        \n",
    "        # img = self.flatten(img)\n",
    "        # img = self.d_img(img)\n",
    "        # img = tf.nn.leaky_relu(img)\n",
    "        \n",
    "        # # concatenate image with paired text\n",
    "        # img_text = tf.concat([text, img], axis=1)\n",
    "        \n",
    "        # logits = self.d(img_text)\n",
    "        # output = tf.nn.sigmoid(logits)\n",
    "        \n",
    "        # Process image\n",
    "        x = self.conv1(img)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.2)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.2)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = tf.nn.leaky_relu(x, alpha=0.2)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # Process text condition\n",
    "        text_processed = self.text_dense(text)\n",
    "\n",
    "        # Concatenate image features and text\n",
    "        combined = tf.concat([x, text_processed], axis=1)\n",
    "\n",
    "        # Final classification\n",
    "        logits = self.final_dense(combined)\n",
    "        # output = tf.nn.sigmoid(logits)\n",
    "        output = logits\n",
    "\n",
    "        return logits, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparas = {\n",
    "    'MAX_SEQ_LENGTH': 20,                     # maximum sequence length\n",
    "    'EMBED_DIM': 256,                         # word embedding dimension\n",
    "    'VOCAB_SIZE': len(word2Id_dict),          # size of dictionary of captions\n",
    "    'RNN_HIDDEN_SIZE': 128,                   # number of RNN neurons\n",
    "    'Z_DIM': 512,                             # random noise z dimension\n",
    "    'DENSE_DIM': 128,                         # number of neurons in dense layer\n",
    "    'IMAGE_SIZE': [64, 64, 3],                # render image size\n",
    "    'BATCH_SIZE': 64,\n",
    "    'LR': 1e-4,\n",
    "    'LR_DECAY': 0.5,\n",
    "    'BETA_1': 0.5,\n",
    "    'N_EPOCH': 1500,                            # number of epoch for demo\n",
    "    'N_SAMPLE': num_training_sample,          # size of training data\n",
    "    'CHECKPOINTS_DIR': './checkpoints/demo',  # checkpoint path\n",
    "    'PRINT_FREQ': 100                           # printing frequency of loss\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = TextEncoder(hparas)\n",
    "generator = Generator(hparas)\n",
    "discriminator = Discriminator(hparas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Pretrain our text encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Image embedding model\n",
    "# base_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL))\n",
    "# pooling_layer = layers.GlobalAveragePooling2D()(base_model.output)\n",
    "# image_model = tf.keras.Model(inputs=base_model.input, outputs=pooling_layer)\n",
    "\n",
    "# text_encoder_optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "\n",
    "# tf.config.run_functions_eagerly(True)\n",
    "# @tf.function\n",
    "# def structured_loss(image_features, text_features):\n",
    "#     # Image-text compatibility scores\n",
    "#     image_features = tf.nn.l2_normalize(image_features, axis=1)\n",
    "#     text_features = tf.nn.l2_normalize(text_features, axis=1)\n",
    "#     scores = tf.reduce_sum(image_features * text_features, axis=1)\n",
    "    \n",
    "#     return tf.reduce_sum(scores)\n",
    "\n",
    "# for epoch in range(100):\n",
    "#     hidden = text_encoder.initialize_hidden_state()\n",
    "#     for image, caption in dataset:\n",
    "#         image_embedding = image_model(image)\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             text_embed, hidden = text_encoder(caption, hidden)\n",
    "#             loss = structured_loss(image_embedding, text_embed)\n",
    "#             print(f\"epoch {epoch}, loss {loss}\")\n",
    "\n",
    "#         grad = tape.gradient(loss, text_encoder.trainable_variables)\n",
    "#         text_encoder_optimizer.apply_gradients(zip(grad, text_encoder.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_logits, fake_logits):\n",
    "    # output value of real image should be 1\n",
    "    # real_loss = cross_entropy(tf.ones_like(real_logits), real_logits)\n",
    "    real_loss = 0.5 * tf.reduce_mean((real_logits - 1) ** 2)\n",
    "    # output value of fake image should be 0\n",
    "    # fake_loss = cross_entropy(tf.zeros_like(fake_logits), fake_logits)\n",
    "    fake_loss = 0.5 * tf.reduce_mean(fake_logits ** 2)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    # output value of fake image should be 0\n",
    "    # return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "    return 0.5 * tf.reduce_mean((fake_output - 1) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use seperated optimizers for training generator and discriminator\n",
    "generator_optimizer = tf.keras.optimizers.Adam(hparas['LR'])\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(hparas['LR'])\n",
    "text_encoder_optimizer = tf.keras.optimizers.Adam(hparas['LR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint restored\n"
     ]
    }
   ],
   "source": [
    "# one benefit of tf.train.Checkpoint() API is we can save everything seperately\n",
    "checkpoint_dir = hparas['CHECKPOINTS_DIR']\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 text_encoder=text_encoder,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator, \n",
    "                                 text_encoder_optimizer=text_encoder_optimizer)\n",
    "# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "# status = checkpoint.restore('./checkpoints/text_encoder/text_encoder-58')\n",
    "status = checkpoint.restore('./checkpoints/demo/ckpt-62')\n",
    "status.assert_existing_objects_matched()\n",
    "print('checkpoint restored')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def image_aug(img):\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    height, width = img.shape[1], img.shape[2]\n",
    "\n",
    "    # random tranlation\n",
    "    dx = tf.random.uniform([], -5, 5, dtype=tf.int32)\n",
    "    dy = tf.random.uniform([], -5, 5, dtype=tf.int32)\n",
    "    img = tf.image.pad_to_bounding_box(\n",
    "        img, \n",
    "        offset_height=tf.maximum(0, dy), \n",
    "        offset_width=tf.maximum(0, dx), \n",
    "        target_height=height + tf.abs(dy), \n",
    "        target_width=width + tf.abs(dx)\n",
    "    )\n",
    "    img = tf.image.crop_to_bounding_box(\n",
    "        img, \n",
    "        offset_height=tf.maximum(0, -dy), \n",
    "        offset_width=tf.maximum(0, -dx), \n",
    "        target_height=height, \n",
    "        target_width=width\n",
    "    )\n",
    "\n",
    "    # random zoom\n",
    "    zoom_factor = tf.random.uniform([], 0.8, 1.2)\n",
    "    new_height, new_width = tf.cast(height * zoom_factor, tf.int32), tf.cast(width * zoom_factor, tf.int32)\n",
    "    img = tf.image.resize(img, [new_height, new_width])\n",
    "    img = tf.image.resize_with_crop_or_pad(img, height, width)\n",
    "\n",
    "    # add noise\n",
    "    noise = tf.random.normal(shape=tf.shape(img), mean=0.0, stddev=0.05, dtype=tf.float32)\n",
    "    img = img + noise\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 64, 3)\n",
      "(64, 64, 64, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for img, caption in dataset.take(1):\n",
    "    print(img.shape)\n",
    "    new_img = image_aug(img)\n",
    "    # print(tf.reduce_max(new_img), tf.reduce_min(new_img))\n",
    "    print(new_img.shape)\n",
    "    plt.imshow(img[0].numpy())\n",
    "    plt.show()\n",
    "    plt.imshow(new_img[0].numpy())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_image, caption, hidden):\n",
    "    # random noise for generator\n",
    "    noise = tf.random.normal(shape=[hparas['BATCH_SIZE'], hparas['Z_DIM']], mean=0.0, stddev=1.0)\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        text_embed, hidden = text_encoder(caption, hidden)\n",
    "        _, fake_image = generator(text_embed, noise)\n",
    "\n",
    "        # Do augmentation for both real and fake images\n",
    "        real_image = image_aug(real_image)\n",
    "        real_image = real_image * 2.0 - 1.0 # normalize to (-1, 1)\n",
    "        fake_image = image_aug(fake_image)\n",
    "\n",
    "        real_logits, real_output = discriminator(real_image, text_embed)\n",
    "        fake_logits, fake_output = discriminator(fake_image, text_embed)\n",
    "\n",
    "        g_loss = generator_loss(fake_logits)\n",
    "        d_loss = discriminator_loss(real_logits, fake_logits)\n",
    "\n",
    "    grad_g = gen_tape.gradient(g_loss, generator.trainable_variables)\n",
    "    grad_d = disc_tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "    # grad_text = gen_tape.gradient(g_loss, text_encoder.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(grad_g, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(grad_d, discriminator.trainable_variables))\n",
    "    # text_encoder_optimizer.apply_gradients(zip(grad_text, text_encoder.trainable_variables))\n",
    "    \n",
    "    return g_loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(caption, noise, hidden):\n",
    "    text_embed, hidden = text_encoder(caption, hidden)\n",
    "    _, fake_image = generator(text_embed, noise)\n",
    "    return fake_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1], 3))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "    return img\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    # getting the pixel values between [0, 1] to save it\n",
    "    return plt.imsave(path, merge(images, size)*0.5 + 0.5)\n",
    "\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(images, size, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_generator(caption, batch_size):\n",
    "    caption = np.asarray(caption)\n",
    "    caption = caption.astype(np.int)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(caption)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-fc27cc894c46>:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  caption = caption.astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "# ni is just calulating grid layout\n",
    "ni = int(np.ceil(np.sqrt(hparas['BATCH_SIZE'])))\n",
    "sample_size = hparas['BATCH_SIZE']\n",
    "sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "sample_sentence = [\"the flower shown has yellow anther red pistil and bright red petals.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has petals that are yellow, white and purple and has dark lines\"] * int(sample_size/ni) + \\\n",
    "                  [\"the petals on this flower are white with a yellow center\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has a lot of small round pink petals.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower is orange in color, and has petals that are ruffled and rounded.\"] * int(sample_size/ni) + \\\n",
    "                  [\"the flower has yellow petals and the center of it is brown.\"] * int(sample_size/ni) + \\\n",
    "                  [\"this flower has petals that are blue and white.\"] * int(sample_size/ni) +\\\n",
    "                  [\"these white flowers have petals that start off white in color and end in a white towards the tips.\"] * int(sample_size/ni)\n",
    "\n",
    "for i, sent in enumerate(sample_sentence):\n",
    "    sample_sentence[i] = sent2IdList(sent)\n",
    "sample_sentence = sample_generator(sample_sentence, hparas['BATCH_SIZE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('samples/demo'):\n",
    "    os.makedirs('samples/demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    # hidden state of RNN\n",
    "    hidden = text_encoder.initialize_hidden_state()\n",
    "    steps_per_epoch = int(hparas['N_SAMPLE']/hparas['BATCH_SIZE'])\n",
    "    \n",
    "    for epoch in range(hparas['N_EPOCH']):\n",
    "        g_total_loss = 0\n",
    "        d_total_loss = 0\n",
    "        start = time.time()\n",
    "        \n",
    "        for image, caption in dataset:\n",
    "            g_loss, d_loss = train_step(image, caption, hidden)\n",
    "            g_total_loss += g_loss\n",
    "            d_total_loss += d_loss\n",
    "            \n",
    "        time_tuple = time.localtime()\n",
    "        time_string = time.strftime(\"%m/%d/%Y, %H:%M:%S\", time_tuple)\n",
    "            \n",
    "        print(\"Epoch {}, gen_loss: {:.4f}, disc_loss: {:.4f}\".format(epoch+1,\n",
    "                                                                     g_total_loss/steps_per_epoch,\n",
    "                                                                     d_total_loss/steps_per_epoch))\n",
    "        print('Time for epoch {} is {:.4f} sec'.format(epoch+1, time.time()-start))\n",
    "        \n",
    "        # save the model\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            save_path = checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "            print(\"Saved checkpoint for epoch {}: {}\".format(epoch+1, save_path))\n",
    "\n",
    "        \n",
    "        # visualization\n",
    "        if (epoch + 1) % hparas['PRINT_FREQ'] == 0:\n",
    "            for caption in sample_sentence:\n",
    "                fake_image = test_step(caption, sample_seed, hidden)\n",
    "            save_images(fake_image, [ni, ni], 'samples/demo/train_{:02d}.jpg'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, gen_loss: 0.4460, disc_loss: 0.0422\n",
      "Time for epoch 1 is 23.1556 sec\n",
      "Epoch 2, gen_loss: 0.4427, disc_loss: 0.0523\n",
      "Time for epoch 2 is 15.5335 sec\n",
      "Epoch 3, gen_loss: 0.4123, disc_loss: 0.0685\n",
      "Time for epoch 3 is 15.4483 sec\n",
      "Epoch 4, gen_loss: 0.4194, disc_loss: 0.0748\n",
      "Time for epoch 4 is 15.3607 sec\n",
      "Epoch 5, gen_loss: 0.4106, disc_loss: 0.0668\n",
      "Time for epoch 5 is 15.1964 sec\n",
      "Epoch 6, gen_loss: 0.4189, disc_loss: 0.0567\n",
      "Time for epoch 6 is 15.1797 sec\n",
      "Epoch 7, gen_loss: 0.4236, disc_loss: 0.0642\n",
      "Time for epoch 7 is 15.2028 sec\n",
      "Epoch 8, gen_loss: 0.4077, disc_loss: 0.0703\n",
      "Time for epoch 8 is 15.2463 sec\n",
      "Epoch 9, gen_loss: 0.4385, disc_loss: 0.0564\n",
      "Time for epoch 9 is 15.2019 sec\n",
      "Epoch 10, gen_loss: 0.4251, disc_loss: 0.0541\n",
      "Time for epoch 10 is 15.2052 sec\n",
      "Epoch 11, gen_loss: 0.4230, disc_loss: 0.0584\n",
      "Time for epoch 11 is 15.3569 sec\n",
      "Epoch 12, gen_loss: 0.4219, disc_loss: 0.0639\n",
      "Time for epoch 12 is 15.5110 sec\n",
      "Epoch 13, gen_loss: 0.4080, disc_loss: 0.0755\n",
      "Time for epoch 13 is 15.5654 sec\n",
      "Epoch 14, gen_loss: 0.4288, disc_loss: 0.0737\n",
      "Time for epoch 14 is 15.4993 sec\n",
      "Epoch 15, gen_loss: 0.4360, disc_loss: 0.0431\n",
      "Time for epoch 15 is 15.5147 sec\n",
      "Epoch 16, gen_loss: 0.4344, disc_loss: 0.0503\n",
      "Time for epoch 16 is 15.5288 sec\n",
      "Epoch 17, gen_loss: 0.4288, disc_loss: 0.0595\n",
      "Time for epoch 17 is 15.5185 sec\n",
      "Epoch 18, gen_loss: 0.4354, disc_loss: 0.0670\n",
      "Time for epoch 18 is 15.5547 sec\n",
      "Epoch 19, gen_loss: 0.4204, disc_loss: 0.0621\n",
      "Time for epoch 19 is 15.4659 sec\n",
      "Epoch 20, gen_loss: 0.4341, disc_loss: 0.0531\n",
      "Time for epoch 20 is 15.5353 sec\n",
      "Epoch 21, gen_loss: 0.4264, disc_loss: 0.0582\n",
      "Time for epoch 21 is 15.5440 sec\n",
      "Epoch 22, gen_loss: 0.4145, disc_loss: 0.0656\n",
      "Time for epoch 22 is 15.5725 sec\n",
      "Epoch 23, gen_loss: 0.4118, disc_loss: 0.0652\n",
      "Time for epoch 23 is 15.5322 sec\n",
      "Epoch 24, gen_loss: 0.4121, disc_loss: 0.0579\n",
      "Time for epoch 24 is 15.5630 sec\n",
      "Epoch 25, gen_loss: 0.4420, disc_loss: 0.0502\n",
      "Time for epoch 25 is 15.5451 sec\n",
      "Epoch 26, gen_loss: 0.4372, disc_loss: 0.0532\n",
      "Time for epoch 26 is 15.5374 sec\n",
      "Epoch 27, gen_loss: 0.4471, disc_loss: 0.0397\n",
      "Time for epoch 27 is 15.5026 sec\n",
      "Epoch 28, gen_loss: 0.4449, disc_loss: 0.0505\n",
      "Time for epoch 28 is 15.6042 sec\n",
      "Epoch 29, gen_loss: 0.4374, disc_loss: 0.0416\n",
      "Time for epoch 29 is 15.5484 sec\n",
      "Epoch 30, gen_loss: 0.4234, disc_loss: 0.0558\n",
      "Time for epoch 30 is 15.5611 sec\n",
      "Epoch 31, gen_loss: 0.4055, disc_loss: 0.0713\n",
      "Time for epoch 31 is 15.5775 sec\n",
      "Epoch 32, gen_loss: 0.4545, disc_loss: 0.0513\n",
      "Time for epoch 32 is 15.5741 sec\n",
      "Epoch 33, gen_loss: 0.4332, disc_loss: 0.0519\n",
      "Time for epoch 33 is 15.5515 sec\n",
      "Epoch 34, gen_loss: 0.4383, disc_loss: 0.0591\n",
      "Time for epoch 34 is 15.5534 sec\n",
      "Epoch 35, gen_loss: 0.4349, disc_loss: 0.0436\n",
      "Time for epoch 35 is 15.5802 sec\n",
      "Epoch 36, gen_loss: 0.4107, disc_loss: 0.0606\n",
      "Time for epoch 36 is 15.5690 sec\n",
      "Epoch 37, gen_loss: 0.4164, disc_loss: 0.0704\n",
      "Time for epoch 37 is 15.5654 sec\n",
      "Epoch 38, gen_loss: 0.4109, disc_loss: 0.0717\n",
      "Time for epoch 38 is 15.5203 sec\n",
      "Epoch 39, gen_loss: 0.4276, disc_loss: 0.0524\n",
      "Time for epoch 39 is 15.5217 sec\n",
      "Epoch 40, gen_loss: 0.4221, disc_loss: 0.0572\n",
      "Time for epoch 40 is 15.4968 sec\n",
      "Epoch 41, gen_loss: 0.4118, disc_loss: 0.0641\n",
      "Time for epoch 41 is 15.5653 sec\n",
      "Epoch 42, gen_loss: 0.4174, disc_loss: 0.0626\n",
      "Time for epoch 42 is 15.5911 sec\n",
      "Epoch 43, gen_loss: 0.4305, disc_loss: 0.0550\n",
      "Time for epoch 43 is 15.5842 sec\n",
      "Epoch 44, gen_loss: 0.4148, disc_loss: 0.0626\n",
      "Time for epoch 44 is 15.5801 sec\n",
      "Epoch 45, gen_loss: 0.4213, disc_loss: 0.0591\n",
      "Time for epoch 45 is 15.5988 sec\n",
      "Epoch 46, gen_loss: 0.4178, disc_loss: 0.0660\n",
      "Time for epoch 46 is 15.5835 sec\n",
      "Epoch 47, gen_loss: 0.4246, disc_loss: 0.0691\n",
      "Time for epoch 47 is 15.6171 sec\n",
      "Epoch 48, gen_loss: 0.4201, disc_loss: 0.0661\n",
      "Time for epoch 48 is 15.5894 sec\n",
      "Epoch 49, gen_loss: 0.4219, disc_loss: 0.0611\n",
      "Time for epoch 49 is 15.6015 sec\n",
      "Epoch 50, gen_loss: 0.4353, disc_loss: 0.0496\n",
      "Time for epoch 50 is 15.5296 sec\n",
      "Epoch 51, gen_loss: 0.4325, disc_loss: 0.0497\n",
      "Time for epoch 51 is 15.6040 sec\n",
      "Epoch 52, gen_loss: 0.4435, disc_loss: 0.0464\n",
      "Time for epoch 52 is 15.2379 sec\n",
      "Epoch 53, gen_loss: 0.4410, disc_loss: 0.0397\n",
      "Time for epoch 53 is 15.2347 sec\n",
      "Epoch 54, gen_loss: 0.4132, disc_loss: 0.0706\n",
      "Time for epoch 54 is 15.2163 sec\n",
      "Epoch 55, gen_loss: 0.4183, disc_loss: 0.0624\n",
      "Time for epoch 55 is 15.3075 sec\n",
      "Epoch 56, gen_loss: 0.4176, disc_loss: 0.0644\n",
      "Time for epoch 56 is 15.2642 sec\n",
      "Epoch 57, gen_loss: 0.4523, disc_loss: 0.0385\n",
      "Time for epoch 57 is 15.2581 sec\n",
      "Epoch 58, gen_loss: 0.4264, disc_loss: 0.0560\n",
      "Time for epoch 58 is 15.2587 sec\n",
      "Epoch 59, gen_loss: 0.4307, disc_loss: 0.0582\n",
      "Time for epoch 59 is 15.2865 sec\n",
      "Epoch 60, gen_loss: 0.4255, disc_loss: 0.0573\n",
      "Time for epoch 60 is 15.2350 sec\n",
      "Epoch 61, gen_loss: 0.4216, disc_loss: 0.0627\n",
      "Time for epoch 61 is 15.2526 sec\n",
      "Epoch 62, gen_loss: 0.4214, disc_loss: 0.0611\n",
      "Time for epoch 62 is 15.2352 sec\n",
      "Epoch 63, gen_loss: 0.4225, disc_loss: 0.0638\n",
      "Time for epoch 63 is 15.3207 sec\n",
      "Epoch 64, gen_loss: 0.4175, disc_loss: 0.0749\n",
      "Time for epoch 64 is 15.2356 sec\n",
      "Epoch 65, gen_loss: 0.4355, disc_loss: 0.0480\n",
      "Time for epoch 65 is 15.2424 sec\n",
      "Epoch 66, gen_loss: 0.4368, disc_loss: 0.0464\n",
      "Time for epoch 66 is 15.2522 sec\n",
      "Epoch 67, gen_loss: 0.4565, disc_loss: 0.0373\n",
      "Time for epoch 67 is 15.2956 sec\n",
      "Epoch 68, gen_loss: 0.4313, disc_loss: 0.0565\n",
      "Time for epoch 68 is 15.2413 sec\n",
      "Epoch 69, gen_loss: 0.4346, disc_loss: 0.0530\n",
      "Time for epoch 69 is 15.2179 sec\n",
      "Epoch 70, gen_loss: 0.4442, disc_loss: 0.0452\n",
      "Time for epoch 70 is 15.2511 sec\n",
      "Epoch 71, gen_loss: 0.4575, disc_loss: 0.0321\n",
      "Time for epoch 71 is 15.2846 sec\n",
      "Epoch 72, gen_loss: 0.4317, disc_loss: 0.0514\n",
      "Time for epoch 72 is 15.2548 sec\n",
      "Epoch 73, gen_loss: 0.4336, disc_loss: 0.0572\n",
      "Time for epoch 73 is 15.2172 sec\n",
      "Epoch 74, gen_loss: 0.4187, disc_loss: 0.0685\n",
      "Time for epoch 74 is 15.2661 sec\n",
      "Epoch 75, gen_loss: 0.4288, disc_loss: 0.0641\n",
      "Time for epoch 75 is 15.2740 sec\n",
      "Epoch 76, gen_loss: 0.4150, disc_loss: 0.0613\n",
      "Time for epoch 76 is 15.2473 sec\n",
      "Epoch 77, gen_loss: 0.4178, disc_loss: 0.0648\n",
      "Time for epoch 77 is 15.2803 sec\n",
      "Epoch 78, gen_loss: 0.4109, disc_loss: 0.0675\n",
      "Time for epoch 78 is 15.2596 sec\n",
      "Epoch 79, gen_loss: 0.4315, disc_loss: 0.0624\n",
      "Time for epoch 79 is 15.2398 sec\n",
      "Epoch 80, gen_loss: 0.4244, disc_loss: 0.0488\n",
      "Time for epoch 80 is 15.2494 sec\n",
      "Epoch 81, gen_loss: 0.4153, disc_loss: 0.0668\n",
      "Time for epoch 81 is 15.2481 sec\n",
      "Epoch 82, gen_loss: 0.4217, disc_loss: 0.0681\n",
      "Time for epoch 82 is 15.2561 sec\n",
      "Epoch 83, gen_loss: 0.4290, disc_loss: 0.0525\n",
      "Time for epoch 83 is 15.2023 sec\n",
      "Epoch 84, gen_loss: 0.4112, disc_loss: 0.0700\n",
      "Time for epoch 84 is 15.2257 sec\n",
      "Epoch 85, gen_loss: 0.4132, disc_loss: 0.0695\n",
      "Time for epoch 85 is 15.2214 sec\n",
      "Epoch 86, gen_loss: 0.4144, disc_loss: 0.0717\n",
      "Time for epoch 86 is 15.2337 sec\n",
      "Epoch 87, gen_loss: 0.4218, disc_loss: 0.0588\n",
      "Time for epoch 87 is 15.2564 sec\n",
      "Epoch 88, gen_loss: 0.4286, disc_loss: 0.0534\n",
      "Time for epoch 88 is 15.2354 sec\n",
      "Epoch 89, gen_loss: 0.4250, disc_loss: 0.0563\n",
      "Time for epoch 89 is 15.2491 sec\n",
      "Epoch 90, gen_loss: 0.4226, disc_loss: 0.0550\n",
      "Time for epoch 90 is 15.2410 sec\n",
      "Epoch 91, gen_loss: 0.4388, disc_loss: 0.0511\n",
      "Time for epoch 91 is 15.5086 sec\n",
      "Epoch 92, gen_loss: 0.4390, disc_loss: 0.0454\n",
      "Time for epoch 92 is 15.7032 sec\n",
      "Epoch 93, gen_loss: 0.4119, disc_loss: 0.0685\n",
      "Time for epoch 93 is 15.5660 sec\n",
      "Epoch 94, gen_loss: 0.4353, disc_loss: 0.0484\n",
      "Time for epoch 94 is 15.6433 sec\n",
      "Epoch 95, gen_loss: 0.4394, disc_loss: 0.0493\n",
      "Time for epoch 95 is 15.6828 sec\n",
      "Epoch 96, gen_loss: 0.4228, disc_loss: 0.0633\n",
      "Time for epoch 96 is 15.5941 sec\n",
      "Epoch 97, gen_loss: 0.4180, disc_loss: 0.0600\n",
      "Time for epoch 97 is 15.6065 sec\n",
      "Epoch 98, gen_loss: 0.4078, disc_loss: 0.0701\n",
      "Time for epoch 98 is 15.3364 sec\n",
      "Epoch 99, gen_loss: 0.4314, disc_loss: 0.0486\n",
      "Time for epoch 99 is 15.3590 sec\n",
      "Epoch 100, gen_loss: 0.4102, disc_loss: 0.0763\n",
      "Time for epoch 100 is 15.3184 sec\n",
      "Saved checkpoint for epoch 100: ./checkpoints/demo/ckpt-63\n",
      "Epoch 101, gen_loss: 0.4093, disc_loss: 0.0632\n",
      "Time for epoch 101 is 15.5768 sec\n",
      "Epoch 102, gen_loss: 0.4343, disc_loss: 0.0619\n",
      "Time for epoch 102 is 15.4546 sec\n",
      "Epoch 103, gen_loss: 0.4151, disc_loss: 0.0607\n",
      "Time for epoch 103 is 15.5842 sec\n",
      "Epoch 104, gen_loss: 0.4407, disc_loss: 0.0486\n",
      "Time for epoch 104 is 15.8449 sec\n",
      "Epoch 105, gen_loss: 0.4307, disc_loss: 0.0464\n",
      "Time for epoch 105 is 15.5568 sec\n",
      "Epoch 106, gen_loss: 0.4396, disc_loss: 0.0530\n",
      "Time for epoch 106 is 15.5606 sec\n",
      "Epoch 107, gen_loss: 0.4207, disc_loss: 0.0604\n",
      "Time for epoch 107 is 15.6019 sec\n",
      "Epoch 108, gen_loss: 0.3910, disc_loss: 0.0770\n",
      "Time for epoch 108 is 15.6002 sec\n",
      "Epoch 109, gen_loss: 0.4094, disc_loss: 0.0830\n",
      "Time for epoch 109 is 15.5670 sec\n",
      "Epoch 110, gen_loss: 0.4304, disc_loss: 0.0483\n",
      "Time for epoch 110 is 15.5584 sec\n",
      "Epoch 111, gen_loss: 0.4446, disc_loss: 0.0385\n",
      "Time for epoch 111 is 15.5837 sec\n",
      "Epoch 112, gen_loss: 0.4340, disc_loss: 0.0528\n",
      "Time for epoch 112 is 15.5808 sec\n",
      "Epoch 113, gen_loss: 0.4243, disc_loss: 0.0503\n",
      "Time for epoch 113 is 15.6051 sec\n",
      "Epoch 114, gen_loss: 0.4195, disc_loss: 0.0686\n",
      "Time for epoch 114 is 15.5810 sec\n",
      "Epoch 115, gen_loss: 0.4309, disc_loss: 0.0611\n",
      "Time for epoch 115 is 15.5806 sec\n",
      "Epoch 116, gen_loss: 0.4476, disc_loss: 0.0469\n",
      "Time for epoch 116 is 15.5980 sec\n",
      "Epoch 117, gen_loss: 0.4270, disc_loss: 0.0549\n",
      "Time for epoch 117 is 15.5962 sec\n",
      "Epoch 118, gen_loss: 0.4368, disc_loss: 0.0601\n",
      "Time for epoch 118 is 15.5933 sec\n",
      "Epoch 119, gen_loss: 0.4258, disc_loss: 0.0527\n",
      "Time for epoch 119 is 15.6347 sec\n",
      "Epoch 120, gen_loss: 0.4282, disc_loss: 0.0563\n",
      "Time for epoch 120 is 15.6177 sec\n",
      "Epoch 121, gen_loss: 0.3941, disc_loss: 0.0793\n",
      "Time for epoch 121 is 15.6091 sec\n",
      "Epoch 122, gen_loss: 0.4306, disc_loss: 0.0545\n",
      "Time for epoch 122 is 15.5641 sec\n",
      "Epoch 123, gen_loss: 0.4353, disc_loss: 0.0520\n",
      "Time for epoch 123 is 15.5768 sec\n",
      "Epoch 124, gen_loss: 0.4118, disc_loss: 0.0698\n",
      "Time for epoch 124 is 15.5982 sec\n",
      "Epoch 125, gen_loss: 0.4136, disc_loss: 0.0681\n",
      "Time for epoch 125 is 15.5941 sec\n",
      "Epoch 126, gen_loss: 0.4226, disc_loss: 0.0591\n",
      "Time for epoch 126 is 15.5887 sec\n",
      "Epoch 127, gen_loss: 0.4473, disc_loss: 0.0451\n",
      "Time for epoch 127 is 15.5958 sec\n",
      "Epoch 128, gen_loss: 0.4264, disc_loss: 0.0569\n",
      "Time for epoch 128 is 15.6371 sec\n",
      "Epoch 129, gen_loss: 0.3933, disc_loss: 0.0793\n",
      "Time for epoch 129 is 15.5548 sec\n",
      "Epoch 130, gen_loss: 0.3939, disc_loss: 0.0864\n",
      "Time for epoch 130 is 15.5942 sec\n",
      "Epoch 131, gen_loss: 0.4156, disc_loss: 0.0636\n",
      "Time for epoch 131 is 15.5886 sec\n",
      "Epoch 132, gen_loss: 0.4438, disc_loss: 0.0412\n",
      "Time for epoch 132 is 15.6014 sec\n",
      "Epoch 133, gen_loss: 0.4374, disc_loss: 0.0487\n",
      "Time for epoch 133 is 15.6167 sec\n",
      "Epoch 134, gen_loss: 0.4318, disc_loss: 0.0580\n",
      "Time for epoch 134 is 15.5751 sec\n",
      "Epoch 135, gen_loss: 0.4241, disc_loss: 0.0538\n",
      "Time for epoch 135 is 15.6041 sec\n",
      "Epoch 136, gen_loss: 0.4358, disc_loss: 0.0490\n",
      "Time for epoch 136 is 15.6391 sec\n",
      "Epoch 137, gen_loss: 0.4378, disc_loss: 0.0488\n",
      "Time for epoch 137 is 15.5573 sec\n",
      "Epoch 138, gen_loss: 0.4327, disc_loss: 0.0505\n",
      "Time for epoch 138 is 15.6420 sec\n",
      "Epoch 139, gen_loss: 0.4297, disc_loss: 0.0513\n",
      "Time for epoch 139 is 15.5941 sec\n",
      "Epoch 140, gen_loss: 0.4256, disc_loss: 0.0507\n",
      "Time for epoch 140 is 15.5911 sec\n",
      "Epoch 141, gen_loss: 0.4242, disc_loss: 0.0577\n",
      "Time for epoch 141 is 15.6045 sec\n",
      "Epoch 142, gen_loss: 0.4246, disc_loss: 0.0530\n",
      "Time for epoch 142 is 15.5670 sec\n",
      "Epoch 143, gen_loss: 0.4361, disc_loss: 0.0489\n",
      "Time for epoch 143 is 15.5965 sec\n",
      "Epoch 144, gen_loss: 0.4165, disc_loss: 0.0657\n",
      "Time for epoch 144 is 15.5828 sec\n",
      "Epoch 145, gen_loss: 0.4241, disc_loss: 0.0615\n",
      "Time for epoch 145 is 15.5348 sec\n",
      "Epoch 146, gen_loss: 0.4336, disc_loss: 0.0492\n",
      "Time for epoch 146 is 15.5963 sec\n",
      "Epoch 147, gen_loss: 0.4104, disc_loss: 0.0649\n",
      "Time for epoch 147 is 15.5755 sec\n",
      "Epoch 148, gen_loss: 0.4127, disc_loss: 0.0638\n",
      "Time for epoch 148 is 15.6137 sec\n",
      "Epoch 149, gen_loss: 0.4453, disc_loss: 0.0474\n",
      "Time for epoch 149 is 15.5408 sec\n",
      "Epoch 150, gen_loss: 0.4064, disc_loss: 0.0767\n",
      "Time for epoch 150 is 15.5887 sec\n",
      "Epoch 151, gen_loss: 0.4026, disc_loss: 0.0700\n",
      "Time for epoch 151 is 15.6018 sec\n",
      "Epoch 152, gen_loss: 0.4280, disc_loss: 0.0519\n",
      "Time for epoch 152 is 15.6066 sec\n",
      "Epoch 153, gen_loss: 0.4310, disc_loss: 0.0546\n",
      "Time for epoch 153 is 15.5953 sec\n",
      "Epoch 154, gen_loss: 0.4378, disc_loss: 0.0524\n",
      "Time for epoch 154 is 15.5708 sec\n",
      "Epoch 155, gen_loss: 0.4454, disc_loss: 0.0394\n",
      "Time for epoch 155 is 15.5654 sec\n",
      "Epoch 156, gen_loss: 0.4504, disc_loss: 0.0365\n",
      "Time for epoch 156 is 15.5974 sec\n",
      "Epoch 157, gen_loss: 0.4326, disc_loss: 0.0534\n",
      "Time for epoch 157 is 15.5635 sec\n",
      "Epoch 158, gen_loss: 0.4169, disc_loss: 0.0810\n",
      "Time for epoch 158 is 15.5686 sec\n",
      "Epoch 159, gen_loss: 0.4208, disc_loss: 0.0634\n",
      "Time for epoch 159 is 15.5628 sec\n",
      "Epoch 160, gen_loss: 0.4080, disc_loss: 0.0653\n",
      "Time for epoch 160 is 15.5896 sec\n",
      "Epoch 161, gen_loss: 0.4406, disc_loss: 0.0451\n",
      "Time for epoch 161 is 15.5679 sec\n",
      "Epoch 162, gen_loss: 0.4057, disc_loss: 0.0688\n",
      "Time for epoch 162 is 15.5903 sec\n",
      "Epoch 163, gen_loss: 0.4102, disc_loss: 0.0753\n",
      "Time for epoch 163 is 15.5742 sec\n",
      "Epoch 164, gen_loss: 0.4145, disc_loss: 0.0556\n",
      "Time for epoch 164 is 15.5514 sec\n",
      "Epoch 165, gen_loss: 0.4305, disc_loss: 0.0547\n",
      "Time for epoch 165 is 15.5374 sec\n",
      "Epoch 166, gen_loss: 0.4114, disc_loss: 0.0665\n",
      "Time for epoch 166 is 15.6303 sec\n",
      "Epoch 167, gen_loss: 0.4017, disc_loss: 0.0788\n",
      "Time for epoch 167 is 15.2819 sec\n",
      "Epoch 168, gen_loss: 0.4116, disc_loss: 0.0602\n",
      "Time for epoch 168 is 15.6077 sec\n",
      "Epoch 169, gen_loss: 0.4193, disc_loss: 0.0597\n",
      "Time for epoch 169 is 15.5807 sec\n",
      "Epoch 170, gen_loss: 0.4008, disc_loss: 0.0794\n",
      "Time for epoch 170 is 15.5641 sec\n",
      "Epoch 171, gen_loss: 0.4083, disc_loss: 0.0691\n",
      "Time for epoch 171 is 15.6113 sec\n",
      "Epoch 172, gen_loss: 0.4413, disc_loss: 0.0397\n",
      "Time for epoch 172 is 15.5870 sec\n",
      "Epoch 173, gen_loss: 0.4290, disc_loss: 0.0549\n",
      "Time for epoch 173 is 15.5754 sec\n",
      "Epoch 174, gen_loss: 0.4228, disc_loss: 0.0603\n",
      "Time for epoch 174 is 15.5920 sec\n",
      "Epoch 175, gen_loss: 0.4440, disc_loss: 0.0497\n",
      "Time for epoch 175 is 15.5988 sec\n",
      "Epoch 176, gen_loss: 0.4421, disc_loss: 0.0565\n",
      "Time for epoch 176 is 15.5743 sec\n",
      "Epoch 177, gen_loss: 0.4033, disc_loss: 0.0696\n",
      "Time for epoch 177 is 15.5967 sec\n",
      "Epoch 178, gen_loss: 0.4337, disc_loss: 0.0547\n",
      "Time for epoch 178 is 15.5992 sec\n",
      "Epoch 179, gen_loss: 0.4198, disc_loss: 0.0550\n",
      "Time for epoch 179 is 15.5098 sec\n",
      "Epoch 180, gen_loss: 0.4356, disc_loss: 0.0565\n",
      "Time for epoch 180 is 15.5183 sec\n",
      "Epoch 181, gen_loss: 0.4291, disc_loss: 0.0547\n",
      "Time for epoch 181 is 15.3424 sec\n",
      "Epoch 182, gen_loss: 0.4227, disc_loss: 0.0574\n",
      "Time for epoch 182 is 15.3143 sec\n",
      "Epoch 183, gen_loss: 0.4276, disc_loss: 0.0515\n",
      "Time for epoch 183 is 15.4003 sec\n",
      "Epoch 184, gen_loss: 0.4207, disc_loss: 0.0547\n",
      "Time for epoch 184 is 15.2984 sec\n",
      "Epoch 185, gen_loss: 0.4231, disc_loss: 0.0619\n",
      "Time for epoch 185 is 15.7154 sec\n",
      "Epoch 186, gen_loss: 0.4163, disc_loss: 0.0693\n",
      "Time for epoch 186 is 15.7083 sec\n",
      "Epoch 187, gen_loss: 0.4257, disc_loss: 0.0608\n",
      "Time for epoch 187 is 15.6573 sec\n",
      "Epoch 188, gen_loss: 0.4344, disc_loss: 0.0537\n",
      "Time for epoch 188 is 15.6648 sec\n",
      "Epoch 189, gen_loss: 0.4352, disc_loss: 0.0473\n",
      "Time for epoch 189 is 15.5688 sec\n",
      "Epoch 190, gen_loss: 0.4327, disc_loss: 0.0488\n",
      "Time for epoch 190 is 15.6591 sec\n",
      "Epoch 191, gen_loss: 0.4253, disc_loss: 0.0556\n",
      "Time for epoch 191 is 15.6513 sec\n",
      "Epoch 192, gen_loss: 0.4135, disc_loss: 0.0643\n",
      "Time for epoch 192 is 15.6510 sec\n",
      "Epoch 193, gen_loss: 0.4322, disc_loss: 0.0525\n",
      "Time for epoch 193 is 15.6164 sec\n",
      "Epoch 194, gen_loss: 0.4482, disc_loss: 0.0429\n",
      "Time for epoch 194 is 15.6404 sec\n",
      "Epoch 195, gen_loss: 0.4044, disc_loss: 0.0628\n",
      "Time for epoch 195 is 15.6378 sec\n",
      "Epoch 196, gen_loss: 0.4254, disc_loss: 0.0572\n",
      "Time for epoch 196 is 15.6261 sec\n",
      "Epoch 197, gen_loss: 0.4200, disc_loss: 0.0618\n",
      "Time for epoch 197 is 15.6481 sec\n",
      "Epoch 198, gen_loss: 0.4142, disc_loss: 0.0638\n",
      "Time for epoch 198 is 15.6601 sec\n",
      "Epoch 199, gen_loss: 0.4303, disc_loss: 0.0543\n",
      "Time for epoch 199 is 15.5814 sec\n",
      "Epoch 200, gen_loss: 0.4332, disc_loss: 0.0527\n",
      "Time for epoch 200 is 15.6521 sec\n",
      "Saved checkpoint for epoch 200: ./checkpoints/demo/ckpt-64\n",
      "Epoch 201, gen_loss: 0.4422, disc_loss: 0.0437\n",
      "Time for epoch 201 is 15.6127 sec\n",
      "Epoch 202, gen_loss: 0.4378, disc_loss: 0.0503\n",
      "Time for epoch 202 is 15.6300 sec\n",
      "Epoch 203, gen_loss: 0.4237, disc_loss: 0.0538\n",
      "Time for epoch 203 is 15.5959 sec\n",
      "Epoch 204, gen_loss: 0.4161, disc_loss: 0.0695\n",
      "Time for epoch 204 is 15.6075 sec\n",
      "Epoch 205, gen_loss: 0.4138, disc_loss: 0.0605\n",
      "Time for epoch 205 is 15.5975 sec\n",
      "Epoch 206, gen_loss: 0.4249, disc_loss: 0.0609\n",
      "Time for epoch 206 is 15.6032 sec\n",
      "Epoch 207, gen_loss: 0.4030, disc_loss: 0.0749\n",
      "Time for epoch 207 is 15.6129 sec\n",
      "Epoch 208, gen_loss: 0.4213, disc_loss: 0.0603\n",
      "Time for epoch 208 is 15.6179 sec\n",
      "Epoch 209, gen_loss: 0.4262, disc_loss: 0.0548\n",
      "Time for epoch 209 is 15.6640 sec\n",
      "Epoch 210, gen_loss: 0.4177, disc_loss: 0.0647\n",
      "Time for epoch 210 is 15.6236 sec\n",
      "Epoch 211, gen_loss: 0.4303, disc_loss: 0.0536\n",
      "Time for epoch 211 is 15.5969 sec\n",
      "Epoch 212, gen_loss: 0.4237, disc_loss: 0.0533\n",
      "Time for epoch 212 is 15.6080 sec\n",
      "Epoch 213, gen_loss: 0.4276, disc_loss: 0.0559\n",
      "Time for epoch 213 is 15.6026 sec\n",
      "Epoch 214, gen_loss: 0.4296, disc_loss: 0.0582\n",
      "Time for epoch 214 is 15.6087 sec\n",
      "Epoch 215, gen_loss: 0.4089, disc_loss: 0.0633\n",
      "Time for epoch 215 is 15.6019 sec\n",
      "Epoch 216, gen_loss: 0.4264, disc_loss: 0.0546\n",
      "Time for epoch 216 is 15.6130 sec\n",
      "Epoch 217, gen_loss: 0.4342, disc_loss: 0.0540\n",
      "Time for epoch 217 is 15.5894 sec\n",
      "Epoch 218, gen_loss: 0.4170, disc_loss: 0.0723\n",
      "Time for epoch 218 is 15.6064 sec\n",
      "Epoch 219, gen_loss: 0.4017, disc_loss: 0.0715\n",
      "Time for epoch 219 is 15.5840 sec\n",
      "Epoch 220, gen_loss: 0.4125, disc_loss: 0.0628\n",
      "Time for epoch 220 is 15.6033 sec\n",
      "Epoch 221, gen_loss: 0.4218, disc_loss: 0.0712\n",
      "Time for epoch 221 is 15.6060 sec\n",
      "Epoch 222, gen_loss: 0.3986, disc_loss: 0.0702\n",
      "Time for epoch 222 is 15.6053 sec\n",
      "Epoch 223, gen_loss: 0.3816, disc_loss: 0.0855\n",
      "Time for epoch 223 is 15.6068 sec\n",
      "Epoch 224, gen_loss: 0.4200, disc_loss: 0.0591\n",
      "Time for epoch 224 is 15.5982 sec\n",
      "Epoch 225, gen_loss: 0.4561, disc_loss: 0.0406\n",
      "Time for epoch 225 is 15.6276 sec\n",
      "Epoch 226, gen_loss: 0.4455, disc_loss: 0.0401\n",
      "Time for epoch 226 is 15.6234 sec\n",
      "Epoch 227, gen_loss: 0.4348, disc_loss: 0.0500\n",
      "Time for epoch 227 is 15.6469 sec\n",
      "Epoch 228, gen_loss: 0.4261, disc_loss: 0.0584\n",
      "Time for epoch 228 is 15.6652 sec\n",
      "Epoch 229, gen_loss: 0.4307, disc_loss: 0.0523\n",
      "Time for epoch 229 is 15.7011 sec\n",
      "Epoch 230, gen_loss: 0.4380, disc_loss: 0.0451\n",
      "Time for epoch 230 is 15.5895 sec\n",
      "Epoch 231, gen_loss: 0.4401, disc_loss: 0.0433\n",
      "Time for epoch 231 is 15.6193 sec\n",
      "Epoch 232, gen_loss: 0.4284, disc_loss: 0.0521\n",
      "Time for epoch 232 is 15.6292 sec\n",
      "Epoch 233, gen_loss: 0.4392, disc_loss: 0.0543\n",
      "Time for epoch 233 is 15.6206 sec\n",
      "Epoch 234, gen_loss: 0.4232, disc_loss: 0.0629\n",
      "Time for epoch 234 is 15.5947 sec\n",
      "Epoch 235, gen_loss: 0.4231, disc_loss: 0.0617\n",
      "Time for epoch 235 is 15.6421 sec\n",
      "Epoch 236, gen_loss: 0.4065, disc_loss: 0.0695\n",
      "Time for epoch 236 is 15.6188 sec\n",
      "Epoch 237, gen_loss: 0.4412, disc_loss: 0.0430\n",
      "Time for epoch 237 is 15.6020 sec\n",
      "Epoch 238, gen_loss: 0.4242, disc_loss: 0.0532\n",
      "Time for epoch 238 is 15.6541 sec\n",
      "Epoch 239, gen_loss: 0.4177, disc_loss: 0.0612\n",
      "Time for epoch 239 is 15.3260 sec\n",
      "Epoch 240, gen_loss: 0.4188, disc_loss: 0.0690\n",
      "Time for epoch 240 is 15.2698 sec\n",
      "Epoch 241, gen_loss: 0.4240, disc_loss: 0.0561\n",
      "Time for epoch 241 is 15.3068 sec\n",
      "Epoch 242, gen_loss: 0.4264, disc_loss: 0.0622\n",
      "Time for epoch 242 is 15.2981 sec\n",
      "Epoch 243, gen_loss: 0.4189, disc_loss: 0.0619\n",
      "Time for epoch 243 is 15.2921 sec\n",
      "Epoch 244, gen_loss: 0.4192, disc_loss: 0.0639\n",
      "Time for epoch 244 is 15.3150 sec\n",
      "Epoch 245, gen_loss: 0.4167, disc_loss: 0.0664\n",
      "Time for epoch 245 is 15.3086 sec\n",
      "Epoch 246, gen_loss: 0.4353, disc_loss: 0.0454\n",
      "Time for epoch 246 is 15.2863 sec\n",
      "Epoch 247, gen_loss: 0.4273, disc_loss: 0.0515\n",
      "Time for epoch 247 is 15.2871 sec\n",
      "Epoch 248, gen_loss: 0.4270, disc_loss: 0.0548\n",
      "Time for epoch 248 is 15.3069 sec\n",
      "Epoch 249, gen_loss: 0.4361, disc_loss: 0.0476\n",
      "Time for epoch 249 is 15.3051 sec\n",
      "Epoch 250, gen_loss: 0.4100, disc_loss: 0.0620\n",
      "Time for epoch 250 is 15.2933 sec\n",
      "Epoch 251, gen_loss: 0.4298, disc_loss: 0.0586\n",
      "Time for epoch 251 is 15.2785 sec\n",
      "Epoch 252, gen_loss: 0.4375, disc_loss: 0.0416\n",
      "Time for epoch 252 is 15.3211 sec\n",
      "Epoch 253, gen_loss: 0.4227, disc_loss: 0.0643\n",
      "Time for epoch 253 is 15.2930 sec\n",
      "Epoch 254, gen_loss: 0.4204, disc_loss: 0.0650\n",
      "Time for epoch 254 is 15.2636 sec\n",
      "Epoch 255, gen_loss: 0.4337, disc_loss: 0.0583\n",
      "Time for epoch 255 is 15.2734 sec\n",
      "Epoch 256, gen_loss: 0.4231, disc_loss: 0.0601\n",
      "Time for epoch 256 is 15.3172 sec\n",
      "Epoch 257, gen_loss: 0.4310, disc_loss: 0.0489\n",
      "Time for epoch 257 is 15.2780 sec\n",
      "Epoch 258, gen_loss: 0.4410, disc_loss: 0.0456\n",
      "Time for epoch 258 is 15.2961 sec\n",
      "Epoch 259, gen_loss: 0.4660, disc_loss: 0.0306\n",
      "Time for epoch 259 is 15.2518 sec\n",
      "Epoch 260, gen_loss: 0.4432, disc_loss: 0.0424\n",
      "Time for epoch 260 is 15.2994 sec\n",
      "Epoch 261, gen_loss: 0.4403, disc_loss: 0.0434\n",
      "Time for epoch 261 is 15.3188 sec\n",
      "Epoch 262, gen_loss: 0.4394, disc_loss: 0.0464\n",
      "Time for epoch 262 is 15.2889 sec\n",
      "Epoch 263, gen_loss: 0.4265, disc_loss: 0.0536\n",
      "Time for epoch 263 is 15.2738 sec\n",
      "Epoch 264, gen_loss: 0.4355, disc_loss: 0.0560\n",
      "Time for epoch 264 is 15.3500 sec\n",
      "Epoch 265, gen_loss: 0.4212, disc_loss: 0.0612\n",
      "Time for epoch 265 is 15.2992 sec\n",
      "Epoch 266, gen_loss: 0.4228, disc_loss: 0.0579\n",
      "Time for epoch 266 is 15.2530 sec\n",
      "Epoch 267, gen_loss: 0.4319, disc_loss: 0.0466\n",
      "Time for epoch 267 is 15.2781 sec\n",
      "Epoch 268, gen_loss: 0.4497, disc_loss: 0.0410\n",
      "Time for epoch 268 is 15.3305 sec\n",
      "Epoch 269, gen_loss: 0.4369, disc_loss: 0.0516\n",
      "Time for epoch 269 is 15.2575 sec\n",
      "Epoch 270, gen_loss: 0.4613, disc_loss: 0.0307\n",
      "Time for epoch 270 is 15.2969 sec\n",
      "Epoch 271, gen_loss: 0.4313, disc_loss: 0.0585\n",
      "Time for epoch 271 is 15.2568 sec\n",
      "Epoch 272, gen_loss: 0.4233, disc_loss: 0.0647\n",
      "Time for epoch 272 is 15.3453 sec\n",
      "Epoch 273, gen_loss: 0.4270, disc_loss: 0.0521\n",
      "Time for epoch 273 is 15.2891 sec\n",
      "Epoch 274, gen_loss: 0.4411, disc_loss: 0.0452\n",
      "Time for epoch 274 is 15.3021 sec\n",
      "Epoch 275, gen_loss: 0.4264, disc_loss: 0.0530\n",
      "Time for epoch 275 is 15.2786 sec\n",
      "Epoch 276, gen_loss: 0.4407, disc_loss: 0.0478\n",
      "Time for epoch 276 is 15.3383 sec\n",
      "Epoch 277, gen_loss: 0.4099, disc_loss: 0.0638\n",
      "Time for epoch 277 is 15.2704 sec\n",
      "Epoch 278, gen_loss: 0.4287, disc_loss: 0.0629\n",
      "Time for epoch 278 is 15.3149 sec\n",
      "Epoch 279, gen_loss: 0.4104, disc_loss: 0.0682\n",
      "Time for epoch 279 is 15.2833 sec\n",
      "Epoch 280, gen_loss: 0.4325, disc_loss: 0.0456\n",
      "Time for epoch 280 is 15.3145 sec\n",
      "Epoch 281, gen_loss: 0.4357, disc_loss: 0.0489\n",
      "Time for epoch 281 is 15.3073 sec\n",
      "Epoch 282, gen_loss: 0.4399, disc_loss: 0.0436\n",
      "Time for epoch 282 is 15.2718 sec\n",
      "Epoch 283, gen_loss: 0.4398, disc_loss: 0.0462\n",
      "Time for epoch 283 is 15.2805 sec\n",
      "Epoch 284, gen_loss: 0.4303, disc_loss: 0.0551\n",
      "Time for epoch 284 is 15.3137 sec\n",
      "Epoch 285, gen_loss: 0.4320, disc_loss: 0.0532\n",
      "Time for epoch 285 is 15.2941 sec\n",
      "Epoch 286, gen_loss: 0.4329, disc_loss: 0.0452\n",
      "Time for epoch 286 is 15.2933 sec\n",
      "Epoch 287, gen_loss: 0.4386, disc_loss: 0.0533\n",
      "Time for epoch 287 is 15.2849 sec\n",
      "Epoch 288, gen_loss: 0.4144, disc_loss: 0.0625\n",
      "Time for epoch 288 is 15.2616 sec\n",
      "Epoch 289, gen_loss: 0.4375, disc_loss: 0.0498\n",
      "Time for epoch 289 is 15.6435 sec\n",
      "Epoch 290, gen_loss: 0.4216, disc_loss: 0.0677\n",
      "Time for epoch 290 is 15.4706 sec\n",
      "Epoch 291, gen_loss: 0.4271, disc_loss: 0.0516\n",
      "Time for epoch 291 is 15.3602 sec\n",
      "Epoch 292, gen_loss: 0.4337, disc_loss: 0.0465\n",
      "Time for epoch 292 is 15.3115 sec\n",
      "Epoch 293, gen_loss: 0.4312, disc_loss: 0.0533\n",
      "Time for epoch 293 is 15.3352 sec\n",
      "Epoch 294, gen_loss: 0.4159, disc_loss: 0.0616\n",
      "Time for epoch 294 is 15.3591 sec\n",
      "Epoch 295, gen_loss: 0.4324, disc_loss: 0.0468\n",
      "Time for epoch 295 is 15.3155 sec\n",
      "Epoch 296, gen_loss: 0.4210, disc_loss: 0.0568\n",
      "Time for epoch 296 is 15.3726 sec\n",
      "Epoch 297, gen_loss: 0.4394, disc_loss: 0.0508\n",
      "Time for epoch 297 is 15.3422 sec\n",
      "Epoch 298, gen_loss: 0.4413, disc_loss: 0.0521\n",
      "Time for epoch 298 is 15.4061 sec\n",
      "Epoch 299, gen_loss: 0.4361, disc_loss: 0.0614\n",
      "Time for epoch 299 is 15.5435 sec\n",
      "Epoch 300, gen_loss: 0.4247, disc_loss: 0.0520\n",
      "Time for epoch 300 is 15.5122 sec\n",
      "Saved checkpoint for epoch 300: ./checkpoints/demo/ckpt-65\n",
      "Epoch 301, gen_loss: 0.4226, disc_loss: 0.0644\n",
      "Time for epoch 301 is 15.5650 sec\n",
      "Epoch 302, gen_loss: 0.4212, disc_loss: 0.0598\n",
      "Time for epoch 302 is 15.4352 sec\n",
      "Epoch 303, gen_loss: 0.4261, disc_loss: 0.0608\n",
      "Time for epoch 303 is 15.8324 sec\n",
      "Epoch 304, gen_loss: 0.4282, disc_loss: 0.0575\n",
      "Time for epoch 304 is 15.6404 sec\n",
      "Epoch 305, gen_loss: 0.4118, disc_loss: 0.0625\n",
      "Time for epoch 305 is 15.7262 sec\n",
      "Epoch 306, gen_loss: 0.4396, disc_loss: 0.0433\n",
      "Time for epoch 306 is 15.7731 sec\n",
      "Epoch 307, gen_loss: 0.4490, disc_loss: 0.0405\n",
      "Time for epoch 307 is 15.7469 sec\n",
      "Epoch 308, gen_loss: 0.4177, disc_loss: 0.0618\n",
      "Time for epoch 308 is 15.6781 sec\n",
      "Epoch 309, gen_loss: 0.4309, disc_loss: 0.0547\n",
      "Time for epoch 309 is 15.6422 sec\n",
      "Epoch 310, gen_loss: 0.4062, disc_loss: 0.0733\n",
      "Time for epoch 310 is 15.5797 sec\n",
      "Epoch 311, gen_loss: 0.4142, disc_loss: 0.0709\n",
      "Time for epoch 311 is 15.4764 sec\n",
      "Epoch 312, gen_loss: 0.4290, disc_loss: 0.0585\n",
      "Time for epoch 312 is 15.4596 sec\n",
      "Epoch 313, gen_loss: 0.4472, disc_loss: 0.0468\n",
      "Time for epoch 313 is 15.3482 sec\n",
      "Epoch 314, gen_loss: 0.4400, disc_loss: 0.0428\n",
      "Time for epoch 314 is 15.2997 sec\n",
      "Epoch 315, gen_loss: 0.4281, disc_loss: 0.0579\n",
      "Time for epoch 315 is 15.2940 sec\n",
      "Epoch 316, gen_loss: 0.4419, disc_loss: 0.0478\n",
      "Time for epoch 316 is 15.3235 sec\n",
      "Epoch 317, gen_loss: 0.4236, disc_loss: 0.0596\n",
      "Time for epoch 317 is 15.3200 sec\n",
      "Epoch 318, gen_loss: 0.4242, disc_loss: 0.0574\n",
      "Time for epoch 318 is 15.2910 sec\n",
      "Epoch 319, gen_loss: 0.4199, disc_loss: 0.0624\n",
      "Time for epoch 319 is 15.3934 sec\n",
      "Epoch 320, gen_loss: 0.4241, disc_loss: 0.0597\n",
      "Time for epoch 320 is 15.4353 sec\n",
      "Epoch 321, gen_loss: 0.4383, disc_loss: 0.0498\n",
      "Time for epoch 321 is 15.6761 sec\n",
      "Epoch 322, gen_loss: 0.4040, disc_loss: 0.0651\n",
      "Time for epoch 322 is 15.6952 sec\n",
      "Epoch 323, gen_loss: 0.4012, disc_loss: 0.0761\n",
      "Time for epoch 323 is 15.7483 sec\n",
      "Epoch 324, gen_loss: 0.4149, disc_loss: 0.0752\n",
      "Time for epoch 324 is 15.8746 sec\n",
      "Epoch 325, gen_loss: 0.4169, disc_loss: 0.0600\n",
      "Time for epoch 325 is 15.7502 sec\n",
      "Epoch 326, gen_loss: 0.4353, disc_loss: 0.0443\n",
      "Time for epoch 326 is 15.7172 sec\n",
      "Epoch 327, gen_loss: 0.4297, disc_loss: 0.0579\n",
      "Time for epoch 327 is 15.9217 sec\n",
      "Epoch 328, gen_loss: 0.4347, disc_loss: 0.0561\n",
      "Time for epoch 328 is 16.1104 sec\n",
      "Epoch 329, gen_loss: 0.4318, disc_loss: 0.0556\n",
      "Time for epoch 329 is 16.0466 sec\n",
      "Epoch 330, gen_loss: 0.4374, disc_loss: 0.0435\n",
      "Time for epoch 330 is 16.0526 sec\n",
      "Epoch 331, gen_loss: 0.4427, disc_loss: 0.0434\n",
      "Time for epoch 331 is 16.1621 sec\n",
      "Epoch 332, gen_loss: 0.4192, disc_loss: 0.0568\n",
      "Time for epoch 332 is 16.1162 sec\n",
      "Epoch 333, gen_loss: 0.4333, disc_loss: 0.0536\n",
      "Time for epoch 333 is 16.1259 sec\n",
      "Epoch 334, gen_loss: 0.4226, disc_loss: 0.0574\n",
      "Time for epoch 334 is 16.1222 sec\n",
      "Epoch 335, gen_loss: 0.4385, disc_loss: 0.0520\n",
      "Time for epoch 335 is 16.0360 sec\n",
      "Epoch 336, gen_loss: 0.4160, disc_loss: 0.0645\n",
      "Time for epoch 336 is 16.0709 sec\n",
      "Epoch 337, gen_loss: 0.4241, disc_loss: 0.0507\n",
      "Time for epoch 337 is 16.0981 sec\n",
      "Epoch 338, gen_loss: 0.4227, disc_loss: 0.0630\n",
      "Time for epoch 338 is 16.1173 sec\n",
      "Epoch 339, gen_loss: 0.4279, disc_loss: 0.0582\n",
      "Time for epoch 339 is 16.0502 sec\n",
      "Epoch 340, gen_loss: 0.4323, disc_loss: 0.0519\n",
      "Time for epoch 340 is 16.1922 sec\n",
      "Epoch 341, gen_loss: 0.4288, disc_loss: 0.0568\n",
      "Time for epoch 341 is 16.1532 sec\n",
      "Epoch 342, gen_loss: 0.4391, disc_loss: 0.0464\n",
      "Time for epoch 342 is 16.1674 sec\n",
      "Epoch 343, gen_loss: 0.4246, disc_loss: 0.0585\n",
      "Time for epoch 343 is 16.1693 sec\n",
      "Epoch 344, gen_loss: 0.4308, disc_loss: 0.0510\n",
      "Time for epoch 344 is 16.0784 sec\n",
      "Epoch 345, gen_loss: 0.4185, disc_loss: 0.0656\n",
      "Time for epoch 345 is 16.0922 sec\n",
      "Epoch 346, gen_loss: 0.4328, disc_loss: 0.0561\n",
      "Time for epoch 346 is 16.0542 sec\n",
      "Epoch 347, gen_loss: 0.4405, disc_loss: 0.0457\n",
      "Time for epoch 347 is 16.0340 sec\n",
      "Epoch 348, gen_loss: 0.4248, disc_loss: 0.0513\n",
      "Time for epoch 348 is 16.1681 sec\n",
      "Epoch 349, gen_loss: 0.4306, disc_loss: 0.0577\n",
      "Time for epoch 349 is 16.1640 sec\n",
      "Epoch 350, gen_loss: 0.4265, disc_loss: 0.0565\n",
      "Time for epoch 350 is 16.1629 sec\n",
      "Epoch 351, gen_loss: 0.4258, disc_loss: 0.0596\n",
      "Time for epoch 351 is 16.1161 sec\n",
      "Epoch 352, gen_loss: 0.4234, disc_loss: 0.0595\n",
      "Time for epoch 352 is 16.1260 sec\n",
      "Epoch 353, gen_loss: 0.4297, disc_loss: 0.0463\n",
      "Time for epoch 353 is 16.2885 sec\n",
      "Epoch 354, gen_loss: 0.4362, disc_loss: 0.0441\n",
      "Time for epoch 354 is 15.9539 sec\n",
      "Epoch 355, gen_loss: 0.4339, disc_loss: 0.0536\n",
      "Time for epoch 355 is 16.0137 sec\n",
      "Epoch 356, gen_loss: 0.4470, disc_loss: 0.0416\n",
      "Time for epoch 356 is 16.0692 sec\n",
      "Epoch 357, gen_loss: 0.4307, disc_loss: 0.0531\n",
      "Time for epoch 357 is 16.0088 sec\n",
      "Epoch 358, gen_loss: 0.4170, disc_loss: 0.0627\n",
      "Time for epoch 358 is 16.0215 sec\n",
      "Epoch 359, gen_loss: 0.4161, disc_loss: 0.0625\n",
      "Time for epoch 359 is 16.0377 sec\n",
      "Epoch 360, gen_loss: 0.4140, disc_loss: 0.0608\n",
      "Time for epoch 360 is 16.0339 sec\n",
      "Epoch 361, gen_loss: 0.4071, disc_loss: 0.0678\n",
      "Time for epoch 361 is 15.9736 sec\n",
      "Epoch 362, gen_loss: 0.4195, disc_loss: 0.0624\n",
      "Time for epoch 362 is 15.9968 sec\n",
      "Epoch 363, gen_loss: 0.4171, disc_loss: 0.0618\n",
      "Time for epoch 363 is 16.0288 sec\n",
      "Epoch 364, gen_loss: 0.4188, disc_loss: 0.0652\n",
      "Time for epoch 364 is 16.0423 sec\n",
      "Epoch 365, gen_loss: 0.4378, disc_loss: 0.0422\n",
      "Time for epoch 365 is 16.0802 sec\n",
      "Epoch 366, gen_loss: 0.4356, disc_loss: 0.0491\n",
      "Time for epoch 366 is 16.0817 sec\n",
      "Epoch 367, gen_loss: 0.4451, disc_loss: 0.0412\n",
      "Time for epoch 367 is 16.0875 sec\n",
      "Epoch 368, gen_loss: 0.4421, disc_loss: 0.0477\n",
      "Time for epoch 368 is 16.1487 sec\n",
      "Epoch 369, gen_loss: 0.4471, disc_loss: 0.0440\n",
      "Time for epoch 369 is 16.1228 sec\n",
      "Epoch 370, gen_loss: 0.4352, disc_loss: 0.0521\n",
      "Time for epoch 370 is 16.1808 sec\n",
      "Epoch 371, gen_loss: 0.4451, disc_loss: 0.0440\n",
      "Time for epoch 371 is 15.9657 sec\n",
      "Epoch 372, gen_loss: 0.4526, disc_loss: 0.0387\n",
      "Time for epoch 372 is 15.7506 sec\n",
      "Epoch 373, gen_loss: 0.4533, disc_loss: 0.0382\n",
      "Time for epoch 373 is 15.6843 sec\n",
      "Epoch 374, gen_loss: 0.4124, disc_loss: 0.0613\n",
      "Time for epoch 374 is 15.4965 sec\n",
      "Epoch 375, gen_loss: 0.4311, disc_loss: 0.0557\n",
      "Time for epoch 375 is 15.8807 sec\n",
      "Epoch 376, gen_loss: 0.4101, disc_loss: 0.0660\n",
      "Time for epoch 376 is 15.7831 sec\n",
      "Epoch 377, gen_loss: 0.4112, disc_loss: 0.0612\n",
      "Time for epoch 377 is 15.6978 sec\n",
      "Epoch 378, gen_loss: 0.4222, disc_loss: 0.0606\n",
      "Time for epoch 378 is 15.7260 sec\n",
      "Epoch 379, gen_loss: 0.4307, disc_loss: 0.0517\n",
      "Time for epoch 379 is 15.6157 sec\n",
      "Epoch 380, gen_loss: 0.4284, disc_loss: 0.0591\n",
      "Time for epoch 380 is 15.7530 sec\n",
      "Epoch 381, gen_loss: 0.4267, disc_loss: 0.0533\n",
      "Time for epoch 381 is 15.6061 sec\n",
      "Epoch 382, gen_loss: 0.4162, disc_loss: 0.0593\n",
      "Time for epoch 382 is 15.7264 sec\n",
      "Epoch 383, gen_loss: 0.4234, disc_loss: 0.0609\n",
      "Time for epoch 383 is 15.6820 sec\n",
      "Epoch 384, gen_loss: 0.4249, disc_loss: 0.0547\n",
      "Time for epoch 384 is 15.7751 sec\n",
      "Epoch 385, gen_loss: 0.4310, disc_loss: 0.0521\n",
      "Time for epoch 385 is 15.7164 sec\n",
      "Epoch 386, gen_loss: 0.4213, disc_loss: 0.0618\n",
      "Time for epoch 386 is 15.8833 sec\n",
      "Epoch 387, gen_loss: 0.4316, disc_loss: 0.0529\n",
      "Time for epoch 387 is 15.6828 sec\n",
      "Epoch 388, gen_loss: 0.4175, disc_loss: 0.0539\n",
      "Time for epoch 388 is 15.7031 sec\n",
      "Epoch 389, gen_loss: 0.4204, disc_loss: 0.0600\n",
      "Time for epoch 389 is 15.7861 sec\n",
      "Epoch 390, gen_loss: 0.4200, disc_loss: 0.0613\n",
      "Time for epoch 390 is 15.7657 sec\n",
      "Epoch 391, gen_loss: 0.4302, disc_loss: 0.0589\n",
      "Time for epoch 391 is 16.1419 sec\n",
      "Epoch 392, gen_loss: 0.4137, disc_loss: 0.0585\n",
      "Time for epoch 392 is 16.1692 sec\n",
      "Epoch 393, gen_loss: 0.4490, disc_loss: 0.0419\n",
      "Time for epoch 393 is 16.0149 sec\n",
      "Epoch 394, gen_loss: 0.4397, disc_loss: 0.0475\n",
      "Time for epoch 394 is 16.0389 sec\n",
      "Epoch 395, gen_loss: 0.4456, disc_loss: 0.0404\n",
      "Time for epoch 395 is 15.7957 sec\n",
      "Epoch 396, gen_loss: 0.4412, disc_loss: 0.0482\n",
      "Time for epoch 396 is 15.9334 sec\n",
      "Epoch 397, gen_loss: 0.4417, disc_loss: 0.0482\n",
      "Time for epoch 397 is 15.9317 sec\n",
      "Epoch 398, gen_loss: 0.4270, disc_loss: 0.0540\n",
      "Time for epoch 398 is 16.1434 sec\n",
      "Epoch 399, gen_loss: 0.4295, disc_loss: 0.0580\n",
      "Time for epoch 399 is 15.9385 sec\n",
      "Epoch 400, gen_loss: 0.4423, disc_loss: 0.0434\n",
      "Time for epoch 400 is 15.9137 sec\n",
      "Saved checkpoint for epoch 400: ./checkpoints/demo/ckpt-66\n",
      "Epoch 401, gen_loss: 0.4059, disc_loss: 0.0691\n",
      "Time for epoch 401 is 15.9222 sec\n",
      "Epoch 402, gen_loss: 0.4366, disc_loss: 0.0557\n",
      "Time for epoch 402 is 15.9020 sec\n",
      "Epoch 403, gen_loss: 0.4260, disc_loss: 0.0509\n",
      "Time for epoch 403 is 15.8883 sec\n",
      "Epoch 404, gen_loss: 0.4399, disc_loss: 0.0476\n",
      "Time for epoch 404 is 15.8319 sec\n",
      "Epoch 405, gen_loss: 0.4334, disc_loss: 0.0582\n",
      "Time for epoch 405 is 15.7033 sec\n",
      "Epoch 406, gen_loss: 0.4055, disc_loss: 0.0598\n",
      "Time for epoch 406 is 15.7259 sec\n",
      "Epoch 407, gen_loss: 0.4308, disc_loss: 0.0581\n",
      "Time for epoch 407 is 15.7037 sec\n",
      "Epoch 408, gen_loss: 0.4125, disc_loss: 0.0661\n",
      "Time for epoch 408 is 15.5990 sec\n",
      "Epoch 409, gen_loss: 0.4152, disc_loss: 0.0659\n",
      "Time for epoch 409 is 15.7301 sec\n",
      "Epoch 410, gen_loss: 0.4559, disc_loss: 0.0406\n",
      "Time for epoch 410 is 15.8220 sec\n",
      "Epoch 411, gen_loss: 0.4323, disc_loss: 0.0560\n",
      "Time for epoch 411 is 15.4185 sec\n",
      "Epoch 412, gen_loss: 0.4502, disc_loss: 0.0503\n",
      "Time for epoch 412 is 15.3470 sec\n",
      "Epoch 413, gen_loss: 0.4386, disc_loss: 0.0454\n",
      "Time for epoch 413 is 15.4349 sec\n",
      "Epoch 414, gen_loss: 0.4514, disc_loss: 0.0415\n",
      "Time for epoch 414 is 15.7243 sec\n",
      "Epoch 415, gen_loss: 0.4075, disc_loss: 0.0678\n",
      "Time for epoch 415 is 15.6958 sec\n",
      "Epoch 416, gen_loss: 0.4175, disc_loss: 0.0675\n",
      "Time for epoch 416 is 15.8482 sec\n",
      "Epoch 417, gen_loss: 0.4312, disc_loss: 0.0512\n",
      "Time for epoch 417 is 15.7952 sec\n",
      "Epoch 418, gen_loss: 0.4162, disc_loss: 0.0577\n",
      "Time for epoch 418 is 15.7322 sec\n",
      "Epoch 419, gen_loss: 0.4221, disc_loss: 0.0612\n",
      "Time for epoch 419 is 15.8875 sec\n",
      "Epoch 420, gen_loss: 0.4300, disc_loss: 0.0485\n",
      "Time for epoch 420 is 15.6924 sec\n",
      "Epoch 421, gen_loss: 0.4352, disc_loss: 0.0511\n",
      "Time for epoch 421 is 15.7491 sec\n",
      "Epoch 422, gen_loss: 0.4408, disc_loss: 0.0417\n",
      "Time for epoch 422 is 15.6236 sec\n",
      "Epoch 423, gen_loss: 0.4364, disc_loss: 0.0493\n",
      "Time for epoch 423 is 15.5165 sec\n",
      "Epoch 424, gen_loss: 0.4401, disc_loss: 0.0485\n",
      "Time for epoch 424 is 15.5110 sec\n",
      "Epoch 425, gen_loss: 0.4457, disc_loss: 0.0404\n",
      "Time for epoch 425 is 15.3193 sec\n",
      "Epoch 426, gen_loss: 0.4385, disc_loss: 0.0440\n",
      "Time for epoch 426 is 15.3920 sec\n",
      "Epoch 427, gen_loss: 0.4276, disc_loss: 0.0539\n",
      "Time for epoch 427 is 15.3873 sec\n",
      "Epoch 428, gen_loss: 0.4080, disc_loss: 0.0663\n",
      "Time for epoch 428 is 15.3671 sec\n",
      "Epoch 429, gen_loss: 0.4354, disc_loss: 0.0528\n",
      "Time for epoch 429 is 15.3159 sec\n",
      "Epoch 430, gen_loss: 0.4432, disc_loss: 0.0424\n",
      "Time for epoch 430 is 15.4135 sec\n",
      "Epoch 431, gen_loss: 0.4379, disc_loss: 0.0464\n",
      "Time for epoch 431 is 15.3210 sec\n",
      "Epoch 432, gen_loss: 0.4565, disc_loss: 0.0435\n",
      "Time for epoch 432 is 15.3238 sec\n",
      "Epoch 433, gen_loss: 0.4238, disc_loss: 0.0603\n",
      "Time for epoch 433 is 15.3688 sec\n",
      "Epoch 434, gen_loss: 0.4338, disc_loss: 0.0524\n",
      "Time for epoch 434 is 15.4341 sec\n",
      "Epoch 435, gen_loss: 0.4294, disc_loss: 0.0468\n",
      "Time for epoch 435 is 15.3178 sec\n",
      "Epoch 436, gen_loss: 0.4053, disc_loss: 0.0747\n",
      "Time for epoch 436 is 15.3500 sec\n",
      "Epoch 437, gen_loss: 0.4351, disc_loss: 0.0416\n",
      "Time for epoch 437 is 15.4645 sec\n",
      "Epoch 438, gen_loss: 0.4626, disc_loss: 0.0396\n",
      "Time for epoch 438 is 15.3563 sec\n",
      "Epoch 439, gen_loss: 0.4290, disc_loss: 0.0540\n",
      "Time for epoch 439 is 15.3156 sec\n",
      "Epoch 440, gen_loss: 0.4180, disc_loss: 0.0619\n",
      "Time for epoch 440 is 15.4206 sec\n",
      "Epoch 441, gen_loss: 0.4388, disc_loss: 0.0536\n",
      "Time for epoch 441 is 15.2959 sec\n",
      "Epoch 442, gen_loss: 0.4489, disc_loss: 0.0353\n",
      "Time for epoch 442 is 15.4385 sec\n",
      "Epoch 443, gen_loss: 0.4418, disc_loss: 0.0455\n",
      "Time for epoch 443 is 15.3245 sec\n",
      "Epoch 444, gen_loss: 0.4293, disc_loss: 0.0578\n",
      "Time for epoch 444 is 15.7080 sec\n",
      "Epoch 445, gen_loss: 0.4312, disc_loss: 0.0509\n",
      "Time for epoch 445 is 15.6637 sec\n",
      "Epoch 446, gen_loss: 0.4324, disc_loss: 0.0482\n",
      "Time for epoch 446 is 15.6983 sec\n",
      "Epoch 447, gen_loss: 0.4478, disc_loss: 0.0434\n",
      "Time for epoch 447 is 15.6932 sec\n",
      "Epoch 448, gen_loss: 0.4179, disc_loss: 0.0625\n",
      "Time for epoch 448 is 15.7478 sec\n",
      "Epoch 449, gen_loss: 0.4183, disc_loss: 0.0577\n",
      "Time for epoch 449 is 15.5983 sec\n",
      "Epoch 450, gen_loss: 0.4336, disc_loss: 0.0480\n",
      "Time for epoch 450 is 15.6588 sec\n",
      "Epoch 451, gen_loss: 0.4445, disc_loss: 0.0449\n",
      "Time for epoch 451 is 15.7761 sec\n",
      "Epoch 452, gen_loss: 0.4467, disc_loss: 0.0419\n",
      "Time for epoch 452 is 15.3594 sec\n",
      "Epoch 453, gen_loss: 0.4370, disc_loss: 0.0548\n",
      "Time for epoch 453 is 15.3241 sec\n",
      "Epoch 454, gen_loss: 0.4205, disc_loss: 0.0593\n",
      "Time for epoch 454 is 15.3715 sec\n",
      "Epoch 455, gen_loss: 0.4167, disc_loss: 0.0645\n",
      "Time for epoch 455 is 15.3879 sec\n",
      "Epoch 456, gen_loss: 0.4220, disc_loss: 0.0545\n",
      "Time for epoch 456 is 15.3340 sec\n",
      "Epoch 457, gen_loss: 0.4325, disc_loss: 0.0595\n",
      "Time for epoch 457 is 15.3172 sec\n",
      "Epoch 458, gen_loss: 0.4375, disc_loss: 0.0478\n",
      "Time for epoch 458 is 15.6970 sec\n",
      "Epoch 459, gen_loss: 0.4321, disc_loss: 0.0536\n",
      "Time for epoch 459 is 15.8355 sec\n",
      "Epoch 460, gen_loss: 0.4388, disc_loss: 0.0498\n",
      "Time for epoch 460 is 15.7888 sec\n",
      "Epoch 461, gen_loss: 0.4290, disc_loss: 0.0592\n",
      "Time for epoch 461 is 15.6751 sec\n",
      "Epoch 462, gen_loss: 0.4423, disc_loss: 0.0458\n",
      "Time for epoch 462 is 15.7303 sec\n",
      "Epoch 463, gen_loss: 0.4277, disc_loss: 0.0518\n",
      "Time for epoch 463 is 15.6509 sec\n",
      "Epoch 464, gen_loss: 0.4218, disc_loss: 0.0595\n",
      "Time for epoch 464 is 15.6059 sec\n",
      "Epoch 465, gen_loss: 0.4285, disc_loss: 0.0649\n",
      "Time for epoch 465 is 15.6843 sec\n",
      "Epoch 466, gen_loss: 0.4416, disc_loss: 0.0401\n",
      "Time for epoch 466 is 15.6547 sec\n",
      "Epoch 467, gen_loss: 0.4304, disc_loss: 0.0528\n",
      "Time for epoch 467 is 15.6614 sec\n",
      "Epoch 468, gen_loss: 0.4251, disc_loss: 0.0525\n",
      "Time for epoch 468 is 15.6768 sec\n",
      "Epoch 469, gen_loss: 0.4212, disc_loss: 0.0627\n",
      "Time for epoch 469 is 15.7145 sec\n",
      "Epoch 470, gen_loss: 0.4283, disc_loss: 0.0546\n",
      "Time for epoch 470 is 15.6546 sec\n",
      "Epoch 471, gen_loss: 0.4078, disc_loss: 0.0634\n",
      "Time for epoch 471 is 15.8583 sec\n",
      "Epoch 472, gen_loss: 0.4462, disc_loss: 0.0461\n",
      "Time for epoch 472 is 15.8804 sec\n",
      "Epoch 473, gen_loss: 0.4440, disc_loss: 0.0435\n",
      "Time for epoch 473 is 15.9912 sec\n",
      "Epoch 474, gen_loss: 0.4286, disc_loss: 0.0443\n",
      "Time for epoch 474 is 15.8981 sec\n",
      "Epoch 475, gen_loss: 0.4374, disc_loss: 0.0502\n",
      "Time for epoch 475 is 16.0838 sec\n",
      "Epoch 476, gen_loss: 0.4360, disc_loss: 0.0448\n",
      "Time for epoch 476 is 16.2283 sec\n",
      "Epoch 477, gen_loss: 0.4366, disc_loss: 0.0569\n",
      "Time for epoch 477 is 16.0983 sec\n",
      "Epoch 478, gen_loss: 0.4267, disc_loss: 0.0544\n",
      "Time for epoch 478 is 16.1464 sec\n",
      "Epoch 479, gen_loss: 0.4430, disc_loss: 0.0425\n",
      "Time for epoch 479 is 16.1263 sec\n",
      "Epoch 480, gen_loss: 0.4362, disc_loss: 0.0479\n",
      "Time for epoch 480 is 16.2438 sec\n",
      "Epoch 481, gen_loss: 0.4334, disc_loss: 0.0503\n",
      "Time for epoch 481 is 16.1214 sec\n",
      "Epoch 482, gen_loss: 0.4257, disc_loss: 0.0551\n",
      "Time for epoch 482 is 16.0741 sec\n",
      "Epoch 483, gen_loss: 0.4167, disc_loss: 0.0656\n",
      "Time for epoch 483 is 16.1987 sec\n",
      "Epoch 484, gen_loss: 0.4410, disc_loss: 0.0521\n",
      "Time for epoch 484 is 16.2619 sec\n",
      "Epoch 485, gen_loss: 0.4218, disc_loss: 0.0624\n",
      "Time for epoch 485 is 16.1295 sec\n",
      "Epoch 486, gen_loss: 0.4230, disc_loss: 0.0578\n",
      "Time for epoch 486 is 16.0478 sec\n",
      "Epoch 487, gen_loss: 0.4157, disc_loss: 0.0661\n",
      "Time for epoch 487 is 16.1198 sec\n",
      "Epoch 488, gen_loss: 0.4274, disc_loss: 0.0581\n",
      "Time for epoch 488 is 15.8624 sec\n",
      "Epoch 489, gen_loss: 0.4213, disc_loss: 0.0641\n",
      "Time for epoch 489 is 15.4487 sec\n",
      "Epoch 490, gen_loss: 0.4282, disc_loss: 0.0588\n",
      "Time for epoch 490 is 15.7007 sec\n",
      "Epoch 491, gen_loss: 0.4310, disc_loss: 0.0474\n",
      "Time for epoch 491 is 15.8431 sec\n",
      "Epoch 492, gen_loss: 0.4280, disc_loss: 0.0563\n",
      "Time for epoch 492 is 15.6435 sec\n",
      "Epoch 493, gen_loss: 0.4078, disc_loss: 0.0645\n",
      "Time for epoch 493 is 15.7307 sec\n",
      "Epoch 494, gen_loss: 0.4245, disc_loss: 0.0633\n",
      "Time for epoch 494 is 15.6545 sec\n",
      "Epoch 495, gen_loss: 0.4274, disc_loss: 0.0584\n",
      "Time for epoch 495 is 15.5960 sec\n",
      "Epoch 496, gen_loss: 0.4315, disc_loss: 0.0566\n",
      "Time for epoch 496 is 15.5961 sec\n",
      "Epoch 497, gen_loss: 0.4112, disc_loss: 0.0751\n",
      "Time for epoch 497 is 15.6190 sec\n",
      "Epoch 498, gen_loss: 0.4414, disc_loss: 0.0442\n",
      "Time for epoch 498 is 15.6257 sec\n",
      "Epoch 499, gen_loss: 0.4257, disc_loss: 0.0508\n",
      "Time for epoch 499 is 15.6602 sec\n",
      "Epoch 500, gen_loss: 0.4267, disc_loss: 0.0494\n",
      "Time for epoch 500 is 15.6692 sec\n",
      "Saved checkpoint for epoch 500: ./checkpoints/demo/ckpt-67\n",
      "Epoch 501, gen_loss: 0.4320, disc_loss: 0.0549\n",
      "Time for epoch 501 is 15.6659 sec\n",
      "Epoch 502, gen_loss: 0.4067, disc_loss: 0.0668\n",
      "Time for epoch 502 is 15.5741 sec\n",
      "Epoch 503, gen_loss: 0.4307, disc_loss: 0.0562\n",
      "Time for epoch 503 is 15.5867 sec\n",
      "Epoch 504, gen_loss: 0.4550, disc_loss: 0.0354\n",
      "Time for epoch 504 is 15.6215 sec\n",
      "Epoch 505, gen_loss: 0.4393, disc_loss: 0.0486\n",
      "Time for epoch 505 is 15.5383 sec\n",
      "Epoch 506, gen_loss: 0.4148, disc_loss: 0.0546\n",
      "Time for epoch 506 is 15.6042 sec\n",
      "Epoch 507, gen_loss: 0.4391, disc_loss: 0.0454\n",
      "Time for epoch 507 is 15.6994 sec\n",
      "Epoch 508, gen_loss: 0.4359, disc_loss: 0.0486\n",
      "Time for epoch 508 is 15.3520 sec\n",
      "Epoch 509, gen_loss: 0.4328, disc_loss: 0.0500\n",
      "Time for epoch 509 is 15.4607 sec\n",
      "Epoch 510, gen_loss: 0.4461, disc_loss: 0.0447\n",
      "Time for epoch 510 is 15.5867 sec\n",
      "Epoch 511, gen_loss: 0.4178, disc_loss: 0.0665\n",
      "Time for epoch 511 is 15.5944 sec\n",
      "Epoch 512, gen_loss: 0.4523, disc_loss: 0.0520\n",
      "Time for epoch 512 is 15.4247 sec\n",
      "Epoch 513, gen_loss: 0.4293, disc_loss: 0.0539\n",
      "Time for epoch 513 is 15.3020 sec\n",
      "Epoch 514, gen_loss: 0.4239, disc_loss: 0.0543\n",
      "Time for epoch 514 is 15.3539 sec\n",
      "Epoch 515, gen_loss: 0.4496, disc_loss: 0.0477\n",
      "Time for epoch 515 is 15.5099 sec\n",
      "Epoch 516, gen_loss: 0.4447, disc_loss: 0.0412\n",
      "Time for epoch 516 is 15.4083 sec\n",
      "Epoch 517, gen_loss: 0.4367, disc_loss: 0.0592\n",
      "Time for epoch 517 is 15.3092 sec\n",
      "Epoch 518, gen_loss: 0.4234, disc_loss: 0.0548\n",
      "Time for epoch 518 is 15.3259 sec\n",
      "Epoch 519, gen_loss: 0.4306, disc_loss: 0.0521\n",
      "Time for epoch 519 is 15.4140 sec\n",
      "Epoch 520, gen_loss: 0.4251, disc_loss: 0.0535\n",
      "Time for epoch 520 is 15.5118 sec\n",
      "Epoch 521, gen_loss: 0.4235, disc_loss: 0.0654\n",
      "Time for epoch 521 is 15.4434 sec\n",
      "Epoch 522, gen_loss: 0.4231, disc_loss: 0.0612\n",
      "Time for epoch 522 is 15.3134 sec\n",
      "Epoch 523, gen_loss: 0.4230, disc_loss: 0.0566\n",
      "Time for epoch 523 is 15.4491 sec\n",
      "Epoch 524, gen_loss: 0.4399, disc_loss: 0.0460\n",
      "Time for epoch 524 is 15.4822 sec\n",
      "Epoch 525, gen_loss: 0.4335, disc_loss: 0.0512\n",
      "Time for epoch 525 is 15.4826 sec\n",
      "Epoch 526, gen_loss: 0.4162, disc_loss: 0.0740\n",
      "Time for epoch 526 is 15.3572 sec\n",
      "Epoch 527, gen_loss: 0.4421, disc_loss: 0.0397\n",
      "Time for epoch 527 is 15.5347 sec\n",
      "Epoch 528, gen_loss: 0.4388, disc_loss: 0.0460\n",
      "Time for epoch 528 is 15.4564 sec\n",
      "Epoch 529, gen_loss: 0.4547, disc_loss: 0.0343\n",
      "Time for epoch 529 is 15.5148 sec\n",
      "Epoch 530, gen_loss: 0.4576, disc_loss: 0.0325\n",
      "Time for epoch 530 is 15.8945 sec\n",
      "Epoch 531, gen_loss: 0.4301, disc_loss: 0.0506\n",
      "Time for epoch 531 is 15.4608 sec\n",
      "Epoch 532, gen_loss: 0.4363, disc_loss: 0.0463\n",
      "Time for epoch 532 is 15.4592 sec\n",
      "Epoch 533, gen_loss: 0.4390, disc_loss: 0.0493\n",
      "Time for epoch 533 is 15.4858 sec\n",
      "Epoch 534, gen_loss: 0.4362, disc_loss: 0.0446\n",
      "Time for epoch 534 is 15.4511 sec\n",
      "Epoch 535, gen_loss: 0.4509, disc_loss: 0.0396\n",
      "Time for epoch 535 is 15.3157 sec\n",
      "Epoch 536, gen_loss: 0.4388, disc_loss: 0.0501\n",
      "Time for epoch 536 is 15.2976 sec\n",
      "Epoch 537, gen_loss: 0.4199, disc_loss: 0.0578\n",
      "Time for epoch 537 is 15.3572 sec\n",
      "Epoch 538, gen_loss: 0.4335, disc_loss: 0.0478\n",
      "Time for epoch 538 is 15.3160 sec\n",
      "Epoch 539, gen_loss: 0.4353, disc_loss: 0.0481\n",
      "Time for epoch 539 is 15.3927 sec\n",
      "Epoch 540, gen_loss: 0.4375, disc_loss: 0.0542\n",
      "Time for epoch 540 is 15.3735 sec\n",
      "Epoch 541, gen_loss: 0.4133, disc_loss: 0.0668\n",
      "Time for epoch 541 is 15.3448 sec\n",
      "Epoch 542, gen_loss: 0.4398, disc_loss: 0.0472\n",
      "Time for epoch 542 is 15.3163 sec\n",
      "Epoch 543, gen_loss: 0.4421, disc_loss: 0.0460\n",
      "Time for epoch 543 is 15.4761 sec\n",
      "Epoch 544, gen_loss: 0.4266, disc_loss: 0.0504\n",
      "Time for epoch 544 is 15.4421 sec\n",
      "Epoch 545, gen_loss: 0.4233, disc_loss: 0.0555\n",
      "Time for epoch 545 is 15.3478 sec\n",
      "Epoch 546, gen_loss: 0.4307, disc_loss: 0.0583\n",
      "Time for epoch 546 is 15.3484 sec\n",
      "Epoch 547, gen_loss: 0.4353, disc_loss: 0.0485\n",
      "Time for epoch 547 is 15.2850 sec\n",
      "Epoch 548, gen_loss: 0.4252, disc_loss: 0.0599\n",
      "Time for epoch 548 is 15.3407 sec\n",
      "Epoch 549, gen_loss: 0.4199, disc_loss: 0.0588\n",
      "Time for epoch 549 is 15.4023 sec\n",
      "Epoch 550, gen_loss: 0.4285, disc_loss: 0.0653\n",
      "Time for epoch 550 is 15.7322 sec\n",
      "Epoch 551, gen_loss: 0.4491, disc_loss: 0.0402\n",
      "Time for epoch 551 is 15.6286 sec\n",
      "Epoch 552, gen_loss: 0.4247, disc_loss: 0.0509\n",
      "Time for epoch 552 is 15.6286 sec\n",
      "Epoch 553, gen_loss: 0.4473, disc_loss: 0.0370\n",
      "Time for epoch 553 is 15.6282 sec\n",
      "Epoch 554, gen_loss: 0.4329, disc_loss: 0.0518\n",
      "Time for epoch 554 is 15.6171 sec\n",
      "Epoch 555, gen_loss: 0.4323, disc_loss: 0.0480\n",
      "Time for epoch 555 is 15.6456 sec\n",
      "Epoch 556, gen_loss: 0.4488, disc_loss: 0.0423\n",
      "Time for epoch 556 is 15.6478 sec\n",
      "Epoch 557, gen_loss: 0.4169, disc_loss: 0.0583\n",
      "Time for epoch 557 is 15.6745 sec\n",
      "Epoch 558, gen_loss: 0.4427, disc_loss: 0.0469\n",
      "Time for epoch 558 is 15.6401 sec\n",
      "Epoch 559, gen_loss: 0.4369, disc_loss: 0.0472\n",
      "Time for epoch 559 is 15.5940 sec\n",
      "Epoch 560, gen_loss: 0.4397, disc_loss: 0.0410\n",
      "Time for epoch 560 is 15.5914 sec\n",
      "Epoch 561, gen_loss: 0.4212, disc_loss: 0.0598\n",
      "Time for epoch 561 is 15.6170 sec\n",
      "Epoch 562, gen_loss: 0.4269, disc_loss: 0.0531\n",
      "Time for epoch 562 is 15.6082 sec\n",
      "Epoch 563, gen_loss: 0.4493, disc_loss: 0.0455\n",
      "Time for epoch 563 is 15.5968 sec\n",
      "Epoch 564, gen_loss: 0.4190, disc_loss: 0.0560\n",
      "Time for epoch 564 is 15.5738 sec\n",
      "Epoch 565, gen_loss: 0.4241, disc_loss: 0.0567\n",
      "Time for epoch 565 is 15.6901 sec\n",
      "Epoch 566, gen_loss: 0.4538, disc_loss: 0.0445\n",
      "Time for epoch 566 is 15.6402 sec\n",
      "Epoch 567, gen_loss: 0.4290, disc_loss: 0.0579\n",
      "Time for epoch 567 is 15.6319 sec\n",
      "Epoch 568, gen_loss: 0.4231, disc_loss: 0.0655\n",
      "Time for epoch 568 is 15.8214 sec\n",
      "Epoch 569, gen_loss: 0.4362, disc_loss: 0.0392\n",
      "Time for epoch 569 is 15.8109 sec\n",
      "Epoch 570, gen_loss: 0.4415, disc_loss: 0.0449\n",
      "Time for epoch 570 is 15.8766 sec\n",
      "Epoch 571, gen_loss: 0.4495, disc_loss: 0.0570\n",
      "Time for epoch 571 is 15.7031 sec\n",
      "Epoch 572, gen_loss: 0.4269, disc_loss: 0.0455\n",
      "Time for epoch 572 is 15.6862 sec\n",
      "Epoch 573, gen_loss: 0.4529, disc_loss: 0.0377\n",
      "Time for epoch 573 is 15.6643 sec\n",
      "Epoch 574, gen_loss: 0.4187, disc_loss: 0.0576\n",
      "Time for epoch 574 is 15.6664 sec\n",
      "Epoch 575, gen_loss: 0.4186, disc_loss: 0.0608\n",
      "Time for epoch 575 is 15.5874 sec\n",
      "Epoch 576, gen_loss: 0.4435, disc_loss: 0.0439\n",
      "Time for epoch 576 is 15.6242 sec\n",
      "Epoch 577, gen_loss: 0.4302, disc_loss: 0.0527\n",
      "Time for epoch 577 is 15.7085 sec\n",
      "Epoch 578, gen_loss: 0.4465, disc_loss: 0.0416\n",
      "Time for epoch 578 is 15.6810 sec\n",
      "Epoch 579, gen_loss: 0.4526, disc_loss: 0.0328\n",
      "Time for epoch 579 is 15.6364 sec\n",
      "Epoch 580, gen_loss: 0.4277, disc_loss: 0.0580\n",
      "Time for epoch 580 is 15.6696 sec\n",
      "Epoch 581, gen_loss: 0.4232, disc_loss: 0.0571\n",
      "Time for epoch 581 is 15.6300 sec\n",
      "Epoch 582, gen_loss: 0.4441, disc_loss: 0.0382\n",
      "Time for epoch 582 is 15.7828 sec\n",
      "Epoch 583, gen_loss: 0.4321, disc_loss: 0.0554\n",
      "Time for epoch 583 is 15.6133 sec\n",
      "Epoch 584, gen_loss: 0.4231, disc_loss: 0.0590\n",
      "Time for epoch 584 is 15.6375 sec\n",
      "Epoch 585, gen_loss: 0.4194, disc_loss: 0.0584\n",
      "Time for epoch 585 is 15.6196 sec\n",
      "Epoch 586, gen_loss: 0.4291, disc_loss: 0.0525\n",
      "Time for epoch 586 is 15.6300 sec\n",
      "Epoch 587, gen_loss: 0.4493, disc_loss: 0.0427\n",
      "Time for epoch 587 is 15.6847 sec\n",
      "Epoch 588, gen_loss: 0.4259, disc_loss: 0.0644\n",
      "Time for epoch 588 is 15.8087 sec\n",
      "Epoch 589, gen_loss: 0.4225, disc_loss: 0.0603\n",
      "Time for epoch 589 is 15.8732 sec\n",
      "Epoch 590, gen_loss: 0.4318, disc_loss: 0.0522\n",
      "Time for epoch 590 is 15.7157 sec\n",
      "Epoch 591, gen_loss: 0.4183, disc_loss: 0.0632\n",
      "Time for epoch 591 is 15.9115 sec\n",
      "Epoch 592, gen_loss: 0.4277, disc_loss: 0.0510\n",
      "Time for epoch 592 is 15.6233 sec\n",
      "Epoch 593, gen_loss: 0.4373, disc_loss: 0.0533\n",
      "Time for epoch 593 is 15.6150 sec\n",
      "Epoch 594, gen_loss: 0.4152, disc_loss: 0.0543\n",
      "Time for epoch 594 is 15.7533 sec\n",
      "Epoch 595, gen_loss: 0.4289, disc_loss: 0.0560\n",
      "Time for epoch 595 is 15.7688 sec\n",
      "Epoch 596, gen_loss: 0.4455, disc_loss: 0.0379\n",
      "Time for epoch 596 is 15.7091 sec\n",
      "Epoch 597, gen_loss: 0.4296, disc_loss: 0.0504\n",
      "Time for epoch 597 is 15.7271 sec\n",
      "Epoch 598, gen_loss: 0.4196, disc_loss: 0.0531\n",
      "Time for epoch 598 is 15.7341 sec\n",
      "Epoch 599, gen_loss: 0.4413, disc_loss: 0.0395\n",
      "Time for epoch 599 is 15.6175 sec\n",
      "Epoch 600, gen_loss: 0.4424, disc_loss: 0.0431\n",
      "Time for epoch 600 is 15.6334 sec\n",
      "Saved checkpoint for epoch 600: ./checkpoints/demo/ckpt-68\n",
      "Epoch 601, gen_loss: 0.4277, disc_loss: 0.0637\n",
      "Time for epoch 601 is 15.6677 sec\n",
      "Epoch 602, gen_loss: 0.4316, disc_loss: 0.0504\n",
      "Time for epoch 602 is 15.6354 sec\n",
      "Epoch 603, gen_loss: 0.4379, disc_loss: 0.0475\n",
      "Time for epoch 603 is 15.6210 sec\n",
      "Epoch 604, gen_loss: 0.4319, disc_loss: 0.0592\n",
      "Time for epoch 604 is 15.6165 sec\n",
      "Epoch 605, gen_loss: 0.4342, disc_loss: 0.0439\n",
      "Time for epoch 605 is 15.7145 sec\n",
      "Epoch 606, gen_loss: 0.4254, disc_loss: 0.0560\n",
      "Time for epoch 606 is 15.7014 sec\n",
      "Epoch 607, gen_loss: 0.4357, disc_loss: 0.0479\n",
      "Time for epoch 607 is 15.6959 sec\n",
      "Epoch 608, gen_loss: 0.4270, disc_loss: 0.0543\n",
      "Time for epoch 608 is 15.6966 sec\n",
      "Epoch 609, gen_loss: 0.4385, disc_loss: 0.0548\n",
      "Time for epoch 609 is 15.5986 sec\n",
      "Epoch 610, gen_loss: 0.4235, disc_loss: 0.0524\n",
      "Time for epoch 610 is 15.6259 sec\n",
      "Epoch 611, gen_loss: 0.4244, disc_loss: 0.0585\n",
      "Time for epoch 611 is 15.6242 sec\n",
      "Epoch 612, gen_loss: 0.4398, disc_loss: 0.0535\n",
      "Time for epoch 612 is 15.6843 sec\n",
      "Epoch 613, gen_loss: 0.4420, disc_loss: 0.0392\n",
      "Time for epoch 613 is 15.6949 sec\n",
      "Epoch 614, gen_loss: 0.4564, disc_loss: 0.0389\n",
      "Time for epoch 614 is 15.6951 sec\n",
      "Epoch 615, gen_loss: 0.4256, disc_loss: 0.0575\n",
      "Time for epoch 615 is 15.6697 sec\n",
      "Epoch 616, gen_loss: 0.4343, disc_loss: 0.0501\n",
      "Time for epoch 616 is 15.5953 sec\n",
      "Epoch 617, gen_loss: 0.4301, disc_loss: 0.0504\n",
      "Time for epoch 617 is 15.6477 sec\n",
      "Epoch 618, gen_loss: 0.4412, disc_loss: 0.0462\n",
      "Time for epoch 618 is 15.6219 sec\n",
      "Epoch 619, gen_loss: 0.4497, disc_loss: 0.0380\n",
      "Time for epoch 619 is 15.6631 sec\n",
      "Epoch 620, gen_loss: 0.4476, disc_loss: 0.0361\n",
      "Time for epoch 620 is 15.6355 sec\n",
      "Epoch 621, gen_loss: 0.4438, disc_loss: 0.0446\n",
      "Time for epoch 621 is 15.5965 sec\n",
      "Epoch 622, gen_loss: 0.4528, disc_loss: 0.0409\n",
      "Time for epoch 622 is 15.6257 sec\n",
      "Epoch 623, gen_loss: 0.4540, disc_loss: 0.0328\n",
      "Time for epoch 623 is 15.6910 sec\n",
      "Epoch 624, gen_loss: 0.4410, disc_loss: 0.0456\n",
      "Time for epoch 624 is 15.7391 sec\n",
      "Epoch 625, gen_loss: 0.4125, disc_loss: 0.0706\n",
      "Time for epoch 625 is 15.6932 sec\n",
      "Epoch 626, gen_loss: 0.4357, disc_loss: 0.0516\n",
      "Time for epoch 626 is 15.6451 sec\n",
      "Epoch 627, gen_loss: 0.4390, disc_loss: 0.0458\n",
      "Time for epoch 627 is 15.6441 sec\n",
      "Epoch 628, gen_loss: 0.4445, disc_loss: 0.0407\n",
      "Time for epoch 628 is 15.6295 sec\n",
      "Epoch 629, gen_loss: 0.4557, disc_loss: 0.0625\n",
      "Time for epoch 629 is 15.6283 sec\n",
      "Epoch 630, gen_loss: 0.4453, disc_loss: 0.0510\n",
      "Time for epoch 630 is 15.6997 sec\n",
      "Epoch 631, gen_loss: 0.4453, disc_loss: 0.0422\n",
      "Time for epoch 631 is 15.7046 sec\n",
      "Epoch 632, gen_loss: 0.4278, disc_loss: 0.0510\n",
      "Time for epoch 632 is 15.6856 sec\n",
      "Epoch 633, gen_loss: 0.4309, disc_loss: 0.0564\n",
      "Time for epoch 633 is 15.5867 sec\n",
      "Epoch 634, gen_loss: 0.4275, disc_loss: 0.0454\n",
      "Time for epoch 634 is 15.6209 sec\n",
      "Epoch 635, gen_loss: 0.4215, disc_loss: 0.0527\n",
      "Time for epoch 635 is 15.6413 sec\n",
      "Epoch 636, gen_loss: 0.4444, disc_loss: 0.0417\n",
      "Time for epoch 636 is 15.6386 sec\n",
      "Epoch 637, gen_loss: 0.4224, disc_loss: 0.0597\n",
      "Time for epoch 637 is 15.6394 sec\n",
      "Epoch 638, gen_loss: 0.4504, disc_loss: 0.0477\n",
      "Time for epoch 638 is 15.7023 sec\n",
      "Epoch 639, gen_loss: 0.4341, disc_loss: 0.0536\n",
      "Time for epoch 639 is 15.2885 sec\n",
      "Epoch 640, gen_loss: 0.4312, disc_loss: 0.0555\n",
      "Time for epoch 640 is 15.2715 sec\n",
      "Epoch 641, gen_loss: 0.4302, disc_loss: 0.0559\n",
      "Time for epoch 641 is 15.2843 sec\n",
      "Epoch 642, gen_loss: 0.4285, disc_loss: 0.0427\n",
      "Time for epoch 642 is 15.2925 sec\n",
      "Epoch 643, gen_loss: 0.4227, disc_loss: 0.0672\n",
      "Time for epoch 643 is 15.2992 sec\n",
      "Epoch 644, gen_loss: 0.4386, disc_loss: 0.0455\n",
      "Time for epoch 644 is 15.2729 sec\n",
      "Epoch 645, gen_loss: 0.4333, disc_loss: 0.0567\n",
      "Time for epoch 645 is 15.2907 sec\n",
      "Epoch 646, gen_loss: 0.4455, disc_loss: 0.0422\n",
      "Time for epoch 646 is 15.2938 sec\n",
      "Epoch 647, gen_loss: 0.4397, disc_loss: 0.0465\n",
      "Time for epoch 647 is 15.2921 sec\n",
      "Epoch 648, gen_loss: 0.4258, disc_loss: 0.0478\n",
      "Time for epoch 648 is 15.3141 sec\n",
      "Epoch 649, gen_loss: 0.4522, disc_loss: 0.0341\n",
      "Time for epoch 649 is 15.2974 sec\n",
      "Epoch 650, gen_loss: 0.4599, disc_loss: 0.0355\n",
      "Time for epoch 650 is 15.3053 sec\n",
      "Epoch 651, gen_loss: 0.4476, disc_loss: 0.0371\n",
      "Time for epoch 651 is 15.3266 sec\n",
      "Epoch 652, gen_loss: 0.4277, disc_loss: 0.0612\n",
      "Time for epoch 652 is 15.2694 sec\n",
      "Epoch 653, gen_loss: 0.4383, disc_loss: 0.0492\n",
      "Time for epoch 653 is 15.3123 sec\n",
      "Epoch 654, gen_loss: 0.4441, disc_loss: 0.0413\n",
      "Time for epoch 654 is 15.3148 sec\n",
      "Epoch 655, gen_loss: 0.4428, disc_loss: 0.0402\n",
      "Time for epoch 655 is 15.3591 sec\n",
      "Epoch 656, gen_loss: 0.4361, disc_loss: 0.0582\n",
      "Time for epoch 656 is 15.2924 sec\n",
      "Epoch 657, gen_loss: 0.4463, disc_loss: 0.0411\n",
      "Time for epoch 657 is 15.2826 sec\n",
      "Epoch 658, gen_loss: 0.4372, disc_loss: 0.0442\n",
      "Time for epoch 658 is 15.2900 sec\n",
      "Epoch 659, gen_loss: 0.4407, disc_loss: 0.0371\n",
      "Time for epoch 659 is 15.3335 sec\n",
      "Epoch 660, gen_loss: 0.4186, disc_loss: 0.0564\n",
      "Time for epoch 660 is 15.2931 sec\n",
      "Epoch 661, gen_loss: 0.4330, disc_loss: 0.0493\n",
      "Time for epoch 661 is 15.2763 sec\n",
      "Epoch 662, gen_loss: 0.4354, disc_loss: 0.0541\n",
      "Time for epoch 662 is 15.2764 sec\n",
      "Epoch 663, gen_loss: 0.4314, disc_loss: 0.0499\n",
      "Time for epoch 663 is 15.3582 sec\n",
      "Epoch 664, gen_loss: 0.4439, disc_loss: 0.0417\n",
      "Time for epoch 664 is 15.2778 sec\n",
      "Epoch 665, gen_loss: 0.4483, disc_loss: 0.0411\n",
      "Time for epoch 665 is 15.2825 sec\n",
      "Epoch 666, gen_loss: 0.4436, disc_loss: 0.0506\n",
      "Time for epoch 666 is 15.4759 sec\n",
      "Epoch 667, gen_loss: 0.4260, disc_loss: 0.0477\n",
      "Time for epoch 667 is 15.3286 sec\n",
      "Epoch 668, gen_loss: 0.4358, disc_loss: 0.0414\n",
      "Time for epoch 668 is 15.3668 sec\n",
      "Epoch 669, gen_loss: 0.4526, disc_loss: 0.0373\n",
      "Time for epoch 669 is 15.4272 sec\n",
      "Epoch 670, gen_loss: 0.4499, disc_loss: 0.0405\n",
      "Time for epoch 670 is 15.6829 sec\n",
      "Epoch 671, gen_loss: 0.4410, disc_loss: 0.0465\n",
      "Time for epoch 671 is 15.6158 sec\n",
      "Epoch 672, gen_loss: 0.4556, disc_loss: 0.0342\n",
      "Time for epoch 672 is 15.6376 sec\n",
      "Epoch 673, gen_loss: 0.4156, disc_loss: 0.0660\n",
      "Time for epoch 673 is 15.8198 sec\n",
      "Epoch 674, gen_loss: 0.4233, disc_loss: 0.0553\n",
      "Time for epoch 674 is 15.6689 sec\n",
      "Epoch 675, gen_loss: 0.4401, disc_loss: 0.0464\n",
      "Time for epoch 675 is 15.6670 sec\n",
      "Epoch 676, gen_loss: 0.4476, disc_loss: 0.0428\n",
      "Time for epoch 676 is 15.7481 sec\n",
      "Epoch 677, gen_loss: 0.4376, disc_loss: 0.0433\n",
      "Time for epoch 677 is 15.9160 sec\n",
      "Epoch 678, gen_loss: 0.4410, disc_loss: 0.0472\n",
      "Time for epoch 678 is 16.0729 sec\n",
      "Epoch 679, gen_loss: 0.4412, disc_loss: 0.0503\n",
      "Time for epoch 679 is 16.0099 sec\n",
      "Epoch 680, gen_loss: 0.4471, disc_loss: 0.0484\n",
      "Time for epoch 680 is 16.1573 sec\n",
      "Epoch 681, gen_loss: 0.4294, disc_loss: 0.0502\n",
      "Time for epoch 681 is 16.1608 sec\n",
      "Epoch 682, gen_loss: 0.4298, disc_loss: 0.0500\n",
      "Time for epoch 682 is 16.1730 sec\n",
      "Epoch 683, gen_loss: 0.4337, disc_loss: 0.0483\n",
      "Time for epoch 683 is 16.1247 sec\n",
      "Epoch 684, gen_loss: 0.4386, disc_loss: 0.0512\n",
      "Time for epoch 684 is 16.0703 sec\n",
      "Epoch 685, gen_loss: 0.4349, disc_loss: 0.0437\n",
      "Time for epoch 685 is 16.1035 sec\n",
      "Epoch 686, gen_loss: 0.4367, disc_loss: 0.0472\n",
      "Time for epoch 686 is 16.1046 sec\n",
      "Epoch 687, gen_loss: 0.4401, disc_loss: 0.0438\n",
      "Time for epoch 687 is 15.9828 sec\n",
      "Epoch 688, gen_loss: 0.4434, disc_loss: 0.0397\n",
      "Time for epoch 688 is 16.0830 sec\n",
      "Epoch 689, gen_loss: 0.4229, disc_loss: 0.0535\n",
      "Time for epoch 689 is 16.0555 sec\n",
      "Epoch 690, gen_loss: 0.4264, disc_loss: 0.0596\n",
      "Time for epoch 690 is 16.0150 sec\n",
      "Epoch 691, gen_loss: 0.4377, disc_loss: 0.0480\n",
      "Time for epoch 691 is 16.1541 sec\n",
      "Epoch 692, gen_loss: 0.4413, disc_loss: 0.0448\n",
      "Time for epoch 692 is 16.0379 sec\n",
      "Epoch 693, gen_loss: 0.4340, disc_loss: 0.0413\n",
      "Time for epoch 693 is 16.0205 sec\n",
      "Epoch 694, gen_loss: 0.4389, disc_loss: 0.0479\n",
      "Time for epoch 694 is 16.0387 sec\n",
      "Epoch 695, gen_loss: 0.4254, disc_loss: 0.0558\n",
      "Time for epoch 695 is 16.0369 sec\n",
      "Epoch 696, gen_loss: 0.4502, disc_loss: 0.0438\n",
      "Time for epoch 696 is 16.0674 sec\n",
      "Epoch 697, gen_loss: 0.4274, disc_loss: 0.0555\n",
      "Time for epoch 697 is 16.1512 sec\n",
      "Epoch 698, gen_loss: 0.4453, disc_loss: 0.0478\n",
      "Time for epoch 698 is 16.1537 sec\n",
      "Epoch 699, gen_loss: 0.4277, disc_loss: 0.0559\n",
      "Time for epoch 699 is 16.1181 sec\n",
      "Epoch 700, gen_loss: 0.4377, disc_loss: 0.0434\n",
      "Time for epoch 700 is 16.1133 sec\n",
      "Saved checkpoint for epoch 700: ./checkpoints/demo/ckpt-69\n",
      "Epoch 701, gen_loss: 0.4521, disc_loss: 0.0446\n",
      "Time for epoch 701 is 16.1158 sec\n",
      "Epoch 702, gen_loss: 0.4560, disc_loss: 0.0318\n",
      "Time for epoch 702 is 16.1391 sec\n",
      "Epoch 703, gen_loss: 0.4502, disc_loss: 0.0379\n",
      "Time for epoch 703 is 16.1277 sec\n",
      "Epoch 704, gen_loss: 0.4460, disc_loss: 0.0410\n",
      "Time for epoch 704 is 16.1480 sec\n",
      "Epoch 705, gen_loss: 0.4364, disc_loss: 0.0480\n",
      "Time for epoch 705 is 16.1457 sec\n",
      "Epoch 706, gen_loss: 0.4283, disc_loss: 0.0583\n",
      "Time for epoch 706 is 16.0034 sec\n",
      "Epoch 707, gen_loss: 0.4347, disc_loss: 0.0480\n",
      "Time for epoch 707 is 15.9865 sec\n",
      "Epoch 708, gen_loss: 0.4459, disc_loss: 0.0399\n",
      "Time for epoch 708 is 16.1009 sec\n",
      "Epoch 709, gen_loss: 0.4593, disc_loss: 0.0302\n",
      "Time for epoch 709 is 16.0767 sec\n",
      "Epoch 710, gen_loss: 0.4262, disc_loss: 0.0524\n",
      "Time for epoch 710 is 16.0787 sec\n",
      "Epoch 711, gen_loss: 0.4178, disc_loss: 0.0657\n",
      "Time for epoch 711 is 16.0934 sec\n",
      "Epoch 712, gen_loss: 0.4383, disc_loss: 0.0496\n",
      "Time for epoch 712 is 16.0744 sec\n",
      "Epoch 713, gen_loss: 0.4299, disc_loss: 0.0533\n",
      "Time for epoch 713 is 15.9698 sec\n",
      "Epoch 714, gen_loss: 0.4280, disc_loss: 0.0547\n",
      "Time for epoch 714 is 16.0245 sec\n",
      "Epoch 715, gen_loss: 0.4304, disc_loss: 0.0492\n",
      "Time for epoch 715 is 16.2664 sec\n",
      "Epoch 716, gen_loss: 0.4379, disc_loss: 0.0541\n",
      "Time for epoch 716 is 16.2201 sec\n",
      "Epoch 717, gen_loss: 0.4532, disc_loss: 0.0334\n",
      "Time for epoch 717 is 16.1788 sec\n",
      "Epoch 718, gen_loss: 0.4318, disc_loss: 0.0587\n",
      "Time for epoch 718 is 16.1579 sec\n",
      "Epoch 719, gen_loss: 0.4309, disc_loss: 0.0579\n",
      "Time for epoch 719 is 16.1721 sec\n",
      "Epoch 720, gen_loss: 0.4252, disc_loss: 0.0585\n",
      "Time for epoch 720 is 16.2346 sec\n",
      "Epoch 721, gen_loss: 0.4427, disc_loss: 0.0400\n",
      "Time for epoch 721 is 15.5245 sec\n",
      "Epoch 722, gen_loss: 0.4460, disc_loss: 0.0438\n",
      "Time for epoch 722 is 15.6105 sec\n",
      "Epoch 723, gen_loss: 0.4404, disc_loss: 0.0438\n",
      "Time for epoch 723 is 15.6613 sec\n",
      "Epoch 724, gen_loss: 0.4437, disc_loss: 0.0467\n",
      "Time for epoch 724 is 15.6361 sec\n",
      "Epoch 725, gen_loss: 0.4422, disc_loss: 0.0449\n",
      "Time for epoch 725 is 15.3971 sec\n",
      "Epoch 726, gen_loss: 0.4413, disc_loss: 0.0463\n",
      "Time for epoch 726 is 15.3785 sec\n",
      "Epoch 727, gen_loss: 0.4414, disc_loss: 0.0495\n",
      "Time for epoch 727 is 15.7301 sec\n",
      "Epoch 728, gen_loss: 0.4355, disc_loss: 0.0472\n",
      "Time for epoch 728 is 15.8260 sec\n",
      "Epoch 729, gen_loss: 0.4230, disc_loss: 0.0633\n",
      "Time for epoch 729 is 15.7457 sec\n",
      "Epoch 730, gen_loss: 0.4490, disc_loss: 0.0401\n",
      "Time for epoch 730 is 15.6334 sec\n",
      "Epoch 731, gen_loss: 0.4634, disc_loss: 0.0324\n",
      "Time for epoch 731 is 15.6561 sec\n",
      "Epoch 732, gen_loss: 0.4589, disc_loss: 0.0267\n",
      "Time for epoch 732 is 15.6316 sec\n",
      "Epoch 733, gen_loss: 0.4554, disc_loss: 0.0334\n",
      "Time for epoch 733 is 15.6345 sec\n",
      "Epoch 734, gen_loss: 0.4337, disc_loss: 0.0535\n",
      "Time for epoch 734 is 15.6879 sec\n",
      "Epoch 735, gen_loss: 0.4497, disc_loss: 0.0393\n",
      "Time for epoch 735 is 15.6126 sec\n",
      "Epoch 736, gen_loss: 0.4128, disc_loss: 0.0606\n",
      "Time for epoch 736 is 15.6379 sec\n",
      "Epoch 737, gen_loss: 0.4314, disc_loss: 0.0532\n",
      "Time for epoch 737 is 15.6812 sec\n",
      "Epoch 738, gen_loss: 0.4393, disc_loss: 0.0429\n",
      "Time for epoch 738 is 15.7459 sec\n",
      "Epoch 739, gen_loss: 0.4428, disc_loss: 0.0434\n",
      "Time for epoch 739 is 15.7444 sec\n",
      "Epoch 740, gen_loss: 0.4629, disc_loss: 0.0294\n",
      "Time for epoch 740 is 15.6754 sec\n",
      "Epoch 741, gen_loss: 0.4279, disc_loss: 0.0539\n",
      "Time for epoch 741 is 15.6932 sec\n",
      "Epoch 742, gen_loss: 0.4313, disc_loss: 0.0499\n",
      "Time for epoch 742 is 15.4706 sec\n",
      "Epoch 743, gen_loss: 0.4424, disc_loss: 0.0438\n",
      "Time for epoch 743 is 15.4123 sec\n",
      "Epoch 744, gen_loss: 0.4470, disc_loss: 0.0431\n",
      "Time for epoch 744 is 15.6505 sec\n",
      "Epoch 745, gen_loss: 0.4399, disc_loss: 0.0402\n",
      "Time for epoch 745 is 15.3262 sec\n",
      "Epoch 746, gen_loss: 0.4508, disc_loss: 0.0424\n",
      "Time for epoch 746 is 15.3188 sec\n",
      "Epoch 747, gen_loss: 0.4537, disc_loss: 0.0415\n",
      "Time for epoch 747 is 15.3493 sec\n",
      "Epoch 748, gen_loss: 0.4463, disc_loss: 0.0506\n",
      "Time for epoch 748 is 15.3176 sec\n",
      "Epoch 749, gen_loss: 0.4366, disc_loss: 0.0460\n",
      "Time for epoch 749 is 15.3464 sec\n",
      "Epoch 750, gen_loss: 0.4261, disc_loss: 0.0639\n",
      "Time for epoch 750 is 15.3573 sec\n",
      "Epoch 751, gen_loss: 0.4314, disc_loss: 0.0491\n",
      "Time for epoch 751 is 15.3208 sec\n",
      "Epoch 752, gen_loss: 0.4245, disc_loss: 0.0541\n",
      "Time for epoch 752 is 15.3343 sec\n",
      "Epoch 753, gen_loss: 0.4354, disc_loss: 0.0597\n",
      "Time for epoch 753 is 15.3409 sec\n",
      "Epoch 754, gen_loss: 0.4279, disc_loss: 0.0516\n",
      "Time for epoch 754 is 15.3112 sec\n",
      "Epoch 755, gen_loss: 0.4270, disc_loss: 0.0547\n",
      "Time for epoch 755 is 15.2911 sec\n",
      "Epoch 756, gen_loss: 0.4240, disc_loss: 0.0577\n",
      "Time for epoch 756 is 15.4909 sec\n",
      "Epoch 757, gen_loss: 0.4416, disc_loss: 0.0441\n",
      "Time for epoch 757 is 15.6594 sec\n",
      "Epoch 758, gen_loss: 0.4519, disc_loss: 0.0359\n",
      "Time for epoch 758 is 15.5431 sec\n",
      "Epoch 759, gen_loss: 0.4613, disc_loss: 0.0277\n",
      "Time for epoch 759 is 15.3808 sec\n",
      "Epoch 760, gen_loss: 0.4244, disc_loss: 0.0561\n",
      "Time for epoch 760 is 15.6675 sec\n",
      "Epoch 761, gen_loss: 0.4323, disc_loss: 0.0490\n",
      "Time for epoch 761 is 15.4541 sec\n",
      "Epoch 762, gen_loss: 0.4470, disc_loss: 0.0467\n",
      "Time for epoch 762 is 15.5444 sec\n",
      "Epoch 763, gen_loss: 0.4482, disc_loss: 0.0368\n",
      "Time for epoch 763 is 15.4163 sec\n",
      "Epoch 764, gen_loss: 0.4427, disc_loss: 0.0438\n",
      "Time for epoch 764 is 15.2901 sec\n",
      "Epoch 765, gen_loss: 0.4459, disc_loss: 0.0411\n",
      "Time for epoch 765 is 15.4149 sec\n",
      "Epoch 766, gen_loss: 0.4324, disc_loss: 0.0487\n",
      "Time for epoch 766 is 15.4638 sec\n",
      "Epoch 767, gen_loss: 0.4371, disc_loss: 0.0523\n",
      "Time for epoch 767 is 15.3067 sec\n",
      "Epoch 768, gen_loss: 0.4307, disc_loss: 0.0477\n",
      "Time for epoch 768 is 15.3383 sec\n",
      "Epoch 769, gen_loss: 0.4317, disc_loss: 0.0482\n",
      "Time for epoch 769 is 15.3356 sec\n",
      "Epoch 770, gen_loss: 0.4448, disc_loss: 0.0481\n",
      "Time for epoch 770 is 15.3592 sec\n",
      "Epoch 771, gen_loss: 0.4467, disc_loss: 0.0460\n",
      "Time for epoch 771 is 15.3340 sec\n",
      "Epoch 772, gen_loss: 0.4427, disc_loss: 0.0440\n",
      "Time for epoch 772 is 15.2923 sec\n",
      "Epoch 773, gen_loss: 0.4529, disc_loss: 0.0425\n",
      "Time for epoch 773 is 15.4930 sec\n",
      "Epoch 774, gen_loss: 0.4406, disc_loss: 0.0385\n",
      "Time for epoch 774 is 15.7424 sec\n",
      "Epoch 775, gen_loss: 0.4392, disc_loss: 0.0470\n",
      "Time for epoch 775 is 15.7820 sec\n",
      "Epoch 776, gen_loss: 0.4483, disc_loss: 0.0401\n",
      "Time for epoch 776 is 16.1731 sec\n",
      "Epoch 777, gen_loss: 0.4576, disc_loss: 0.0314\n",
      "Time for epoch 777 is 16.1096 sec\n",
      "Epoch 778, gen_loss: 0.4423, disc_loss: 0.0467\n",
      "Time for epoch 778 is 15.8885 sec\n",
      "Epoch 779, gen_loss: 0.4396, disc_loss: 0.0462\n",
      "Time for epoch 779 is 15.8809 sec\n",
      "Epoch 780, gen_loss: 0.4390, disc_loss: 0.0435\n",
      "Time for epoch 780 is 15.8165 sec\n",
      "Epoch 781, gen_loss: 0.4317, disc_loss: 0.0552\n",
      "Time for epoch 781 is 15.8008 sec\n",
      "Epoch 782, gen_loss: 0.4441, disc_loss: 0.0419\n",
      "Time for epoch 782 is 15.8574 sec\n",
      "Epoch 783, gen_loss: 0.4187, disc_loss: 0.0560\n",
      "Time for epoch 783 is 15.9652 sec\n",
      "Epoch 784, gen_loss: 0.4312, disc_loss: 0.0605\n",
      "Time for epoch 784 is 15.9819 sec\n",
      "Epoch 785, gen_loss: 0.4236, disc_loss: 0.0571\n",
      "Time for epoch 785 is 15.9277 sec\n",
      "Epoch 786, gen_loss: 0.4222, disc_loss: 0.0550\n",
      "Time for epoch 786 is 15.8936 sec\n",
      "Epoch 787, gen_loss: 0.4447, disc_loss: 0.0455\n",
      "Time for epoch 787 is 16.0411 sec\n",
      "Epoch 788, gen_loss: 0.4299, disc_loss: 0.0560\n",
      "Time for epoch 788 is 15.7955 sec\n",
      "Epoch 789, gen_loss: 0.4350, disc_loss: 0.0510\n",
      "Time for epoch 789 is 15.8020 sec\n",
      "Epoch 790, gen_loss: 0.4232, disc_loss: 0.0572\n",
      "Time for epoch 790 is 15.9597 sec\n",
      "Epoch 791, gen_loss: 0.4508, disc_loss: 0.0398\n",
      "Time for epoch 791 is 15.8803 sec\n",
      "Epoch 792, gen_loss: 0.4350, disc_loss: 0.0423\n",
      "Time for epoch 792 is 15.9179 sec\n",
      "Epoch 793, gen_loss: 0.4371, disc_loss: 0.0635\n",
      "Time for epoch 793 is 15.8144 sec\n",
      "Epoch 794, gen_loss: 0.4236, disc_loss: 0.0559\n",
      "Time for epoch 794 is 15.7987 sec\n",
      "Epoch 795, gen_loss: 0.4134, disc_loss: 0.0594\n",
      "Time for epoch 795 is 15.9026 sec\n",
      "Epoch 796, gen_loss: 0.4379, disc_loss: 0.0495\n",
      "Time for epoch 796 is 15.9488 sec\n",
      "Epoch 797, gen_loss: 0.4516, disc_loss: 0.0420\n",
      "Time for epoch 797 is 15.8794 sec\n",
      "Epoch 798, gen_loss: 0.4555, disc_loss: 0.0402\n",
      "Time for epoch 798 is 15.8226 sec\n",
      "Epoch 799, gen_loss: 0.4361, disc_loss: 0.0533\n",
      "Time for epoch 799 is 15.9969 sec\n",
      "Epoch 800, gen_loss: 0.4378, disc_loss: 0.0483\n",
      "Time for epoch 800 is 15.8273 sec\n",
      "Saved checkpoint for epoch 800: ./checkpoints/demo/ckpt-70\n",
      "Epoch 801, gen_loss: 0.4332, disc_loss: 0.0535\n",
      "Time for epoch 801 is 15.8904 sec\n",
      "Epoch 802, gen_loss: 0.4202, disc_loss: 0.0653\n",
      "Time for epoch 802 is 15.8720 sec\n",
      "Epoch 803, gen_loss: 0.4133, disc_loss: 0.0598\n",
      "Time for epoch 803 is 15.6821 sec\n",
      "Epoch 804, gen_loss: 0.4394, disc_loss: 0.0463\n",
      "Time for epoch 804 is 15.8037 sec\n",
      "Epoch 805, gen_loss: 0.4319, disc_loss: 0.0498\n",
      "Time for epoch 805 is 15.7035 sec\n",
      "Epoch 806, gen_loss: 0.4518, disc_loss: 0.0395\n",
      "Time for epoch 806 is 15.5679 sec\n",
      "Epoch 807, gen_loss: 0.4391, disc_loss: 0.0473\n",
      "Time for epoch 807 is 15.5044 sec\n",
      "Epoch 808, gen_loss: 0.4558, disc_loss: 0.0348\n",
      "Time for epoch 808 is 15.5222 sec\n",
      "Epoch 809, gen_loss: 0.4240, disc_loss: 0.0597\n",
      "Time for epoch 809 is 15.5844 sec\n",
      "Epoch 810, gen_loss: 0.4396, disc_loss: 0.0433\n",
      "Time for epoch 810 is 15.5733 sec\n",
      "Epoch 811, gen_loss: 0.4436, disc_loss: 0.0382\n",
      "Time for epoch 811 is 15.5761 sec\n",
      "Epoch 812, gen_loss: 0.4474, disc_loss: 0.0393\n",
      "Time for epoch 812 is 15.7079 sec\n",
      "Epoch 813, gen_loss: 0.4424, disc_loss: 0.0488\n",
      "Time for epoch 813 is 15.6001 sec\n",
      "Epoch 814, gen_loss: 0.4250, disc_loss: 0.0507\n",
      "Time for epoch 814 is 16.1222 sec\n",
      "Epoch 815, gen_loss: 0.4395, disc_loss: 0.0460\n",
      "Time for epoch 815 is 16.0121 sec\n",
      "Epoch 816, gen_loss: 0.4418, disc_loss: 0.0437\n",
      "Time for epoch 816 is 15.9911 sec\n",
      "Epoch 817, gen_loss: 0.4270, disc_loss: 0.0581\n",
      "Time for epoch 817 is 15.9796 sec\n",
      "Epoch 818, gen_loss: 0.4479, disc_loss: 0.0429\n",
      "Time for epoch 818 is 16.0458 sec\n",
      "Epoch 819, gen_loss: 0.4653, disc_loss: 0.0251\n",
      "Time for epoch 819 is 15.9871 sec\n",
      "Epoch 820, gen_loss: 0.4440, disc_loss: 0.0457\n",
      "Time for epoch 820 is 16.1257 sec\n",
      "Epoch 821, gen_loss: 0.4343, disc_loss: 0.0511\n",
      "Time for epoch 821 is 15.9810 sec\n",
      "Epoch 822, gen_loss: 0.4280, disc_loss: 0.0563\n",
      "Time for epoch 822 is 16.1518 sec\n",
      "Epoch 823, gen_loss: 0.4337, disc_loss: 0.0481\n",
      "Time for epoch 823 is 16.0249 sec\n",
      "Epoch 824, gen_loss: 0.4372, disc_loss: 0.0418\n",
      "Time for epoch 824 is 16.0329 sec\n",
      "Epoch 825, gen_loss: 0.4235, disc_loss: 0.0545\n",
      "Time for epoch 825 is 16.1455 sec\n",
      "Epoch 826, gen_loss: 0.4391, disc_loss: 0.0474\n",
      "Time for epoch 826 is 15.9379 sec\n",
      "Epoch 827, gen_loss: 0.4355, disc_loss: 0.0529\n",
      "Time for epoch 827 is 15.8475 sec\n",
      "Epoch 828, gen_loss: 0.4427, disc_loss: 0.0416\n",
      "Time for epoch 828 is 16.0282 sec\n",
      "Epoch 829, gen_loss: 0.4439, disc_loss: 0.0436\n",
      "Time for epoch 829 is 16.0801 sec\n",
      "Epoch 830, gen_loss: 0.4529, disc_loss: 0.0354\n",
      "Time for epoch 830 is 16.1299 sec\n",
      "Epoch 831, gen_loss: 0.4426, disc_loss: 0.0404\n",
      "Time for epoch 831 is 16.1246 sec\n",
      "Epoch 832, gen_loss: 0.4543, disc_loss: 0.0336\n",
      "Time for epoch 832 is 16.1206 sec\n",
      "Epoch 833, gen_loss: 0.4254, disc_loss: 0.0563\n",
      "Time for epoch 833 is 15.9875 sec\n",
      "Epoch 834, gen_loss: 0.4439, disc_loss: 0.0389\n",
      "Time for epoch 834 is 15.9084 sec\n",
      "Epoch 835, gen_loss: 0.4374, disc_loss: 0.0479\n",
      "Time for epoch 835 is 16.0807 sec\n",
      "Epoch 836, gen_loss: 0.4245, disc_loss: 0.0571\n",
      "Time for epoch 836 is 16.0929 sec\n",
      "Epoch 837, gen_loss: 0.4886, disc_loss: 0.0372\n",
      "Time for epoch 837 is 16.0530 sec\n",
      "Epoch 838, gen_loss: 0.4469, disc_loss: 0.0380\n",
      "Time for epoch 838 is 16.0035 sec\n",
      "Epoch 839, gen_loss: 0.4521, disc_loss: 0.0387\n",
      "Time for epoch 839 is 16.1050 sec\n",
      "Epoch 840, gen_loss: 0.4457, disc_loss: 0.0360\n",
      "Time for epoch 840 is 16.3023 sec\n",
      "Epoch 841, gen_loss: 0.4424, disc_loss: 0.0406\n",
      "Time for epoch 841 is 16.2603 sec\n",
      "Epoch 842, gen_loss: 0.4374, disc_loss: 0.0510\n",
      "Time for epoch 842 is 16.1821 sec\n",
      "Epoch 843, gen_loss: 0.4527, disc_loss: 0.0424\n",
      "Time for epoch 843 is 16.1886 sec\n",
      "Epoch 844, gen_loss: 0.4382, disc_loss: 0.0557\n",
      "Time for epoch 844 is 16.1525 sec\n",
      "Epoch 845, gen_loss: 0.4425, disc_loss: 0.0485\n",
      "Time for epoch 845 is 16.2679 sec\n",
      "Epoch 846, gen_loss: 0.4368, disc_loss: 0.0453\n",
      "Time for epoch 846 is 16.2981 sec\n",
      "Epoch 847, gen_loss: 0.4353, disc_loss: 0.0469\n",
      "Time for epoch 847 is 16.0526 sec\n",
      "Epoch 848, gen_loss: 0.4457, disc_loss: 0.0387\n",
      "Time for epoch 848 is 16.0434 sec\n",
      "Epoch 849, gen_loss: 0.4496, disc_loss: 0.0383\n",
      "Time for epoch 849 is 16.0208 sec\n",
      "Epoch 850, gen_loss: 0.4325, disc_loss: 0.0499\n",
      "Time for epoch 850 is 16.1040 sec\n",
      "Epoch 851, gen_loss: 0.4271, disc_loss: 0.0527\n",
      "Time for epoch 851 is 16.1919 sec\n",
      "Epoch 852, gen_loss: 0.4443, disc_loss: 0.0475\n",
      "Time for epoch 852 is 15.9929 sec\n",
      "Epoch 853, gen_loss: 0.4401, disc_loss: 0.0463\n",
      "Time for epoch 853 is 16.0106 sec\n",
      "Epoch 854, gen_loss: 0.4459, disc_loss: 0.0420\n",
      "Time for epoch 854 is 16.0565 sec\n",
      "Epoch 855, gen_loss: 0.4296, disc_loss: 0.0522\n",
      "Time for epoch 855 is 16.1188 sec\n",
      "Epoch 856, gen_loss: 0.4352, disc_loss: 0.0468\n",
      "Time for epoch 856 is 16.3671 sec\n",
      "Epoch 857, gen_loss: 0.4451, disc_loss: 0.0432\n",
      "Time for epoch 857 is 16.0106 sec\n",
      "Epoch 858, gen_loss: 0.4439, disc_loss: 0.0416\n",
      "Time for epoch 858 is 16.1077 sec\n",
      "Epoch 859, gen_loss: 0.4533, disc_loss: 0.0369\n",
      "Time for epoch 859 is 15.9426 sec\n",
      "Epoch 860, gen_loss: 0.4368, disc_loss: 0.0475\n",
      "Time for epoch 860 is 15.9193 sec\n",
      "Epoch 861, gen_loss: 0.4455, disc_loss: 0.0500\n",
      "Time for epoch 861 is 15.9068 sec\n",
      "Epoch 862, gen_loss: 0.4236, disc_loss: 0.0494\n",
      "Time for epoch 862 is 15.8658 sec\n",
      "Epoch 863, gen_loss: 0.4565, disc_loss: 0.0352\n",
      "Time for epoch 863 is 15.9797 sec\n",
      "Epoch 864, gen_loss: 0.4611, disc_loss: 0.0281\n",
      "Time for epoch 864 is 15.7752 sec\n",
      "Epoch 865, gen_loss: 0.4665, disc_loss: 0.0292\n",
      "Time for epoch 865 is 15.7486 sec\n",
      "Epoch 866, gen_loss: 0.4322, disc_loss: 0.0505\n",
      "Time for epoch 866 is 15.8395 sec\n",
      "Epoch 867, gen_loss: 0.4323, disc_loss: 0.0497\n",
      "Time for epoch 867 is 15.7334 sec\n",
      "Epoch 868, gen_loss: 0.4518, disc_loss: 0.0405\n",
      "Time for epoch 868 is 15.8377 sec\n",
      "Epoch 869, gen_loss: 0.4462, disc_loss: 0.0480\n",
      "Time for epoch 869 is 16.0513 sec\n",
      "Epoch 870, gen_loss: 0.4373, disc_loss: 0.0465\n",
      "Time for epoch 870 is 15.8518 sec\n",
      "Epoch 871, gen_loss: 0.4550, disc_loss: 0.0364\n",
      "Time for epoch 871 is 15.8744 sec\n",
      "Epoch 872, gen_loss: 0.4419, disc_loss: 0.0446\n",
      "Time for epoch 872 is 15.7896 sec\n",
      "Epoch 873, gen_loss: 0.4540, disc_loss: 0.0390\n",
      "Time for epoch 873 is 15.9515 sec\n",
      "Epoch 874, gen_loss: 0.4513, disc_loss: 0.0383\n",
      "Time for epoch 874 is 15.8681 sec\n",
      "Epoch 875, gen_loss: 0.4323, disc_loss: 0.0557\n",
      "Time for epoch 875 is 15.8216 sec\n",
      "Epoch 876, gen_loss: 0.4301, disc_loss: 0.0469\n",
      "Time for epoch 876 is 15.9449 sec\n",
      "Epoch 877, gen_loss: 0.4554, disc_loss: 0.0352\n",
      "Time for epoch 877 is 15.9231 sec\n",
      "Epoch 878, gen_loss: 0.4496, disc_loss: 0.0395\n",
      "Time for epoch 878 is 15.9078 sec\n",
      "Epoch 879, gen_loss: 0.4455, disc_loss: 0.0409\n",
      "Time for epoch 879 is 15.8711 sec\n",
      "Epoch 880, gen_loss: 0.4483, disc_loss: 0.0515\n",
      "Time for epoch 880 is 15.9260 sec\n",
      "Epoch 881, gen_loss: 0.4527, disc_loss: 0.0470\n",
      "Time for epoch 881 is 16.0256 sec\n",
      "Epoch 882, gen_loss: 0.4307, disc_loss: 0.0429\n",
      "Time for epoch 882 is 16.1514 sec\n",
      "Epoch 883, gen_loss: 0.4273, disc_loss: 0.0507\n",
      "Time for epoch 883 is 15.9628 sec\n",
      "Epoch 884, gen_loss: 0.4440, disc_loss: 0.0438\n",
      "Time for epoch 884 is 15.9397 sec\n",
      "Epoch 885, gen_loss: 0.4516, disc_loss: 0.0331\n",
      "Time for epoch 885 is 15.7806 sec\n",
      "Epoch 886, gen_loss: 0.4487, disc_loss: 0.0410\n",
      "Time for epoch 886 is 16.0948 sec\n",
      "Epoch 887, gen_loss: 0.4368, disc_loss: 0.0473\n",
      "Time for epoch 887 is 15.9486 sec\n",
      "Epoch 888, gen_loss: 0.4481, disc_loss: 0.0412\n",
      "Time for epoch 888 is 15.9457 sec\n",
      "Epoch 889, gen_loss: 0.4420, disc_loss: 0.0445\n",
      "Time for epoch 889 is 16.0014 sec\n",
      "Epoch 890, gen_loss: 0.4474, disc_loss: 0.0533\n",
      "Time for epoch 890 is 16.2147 sec\n",
      "Epoch 891, gen_loss: 0.4490, disc_loss: 0.0422\n",
      "Time for epoch 891 is 16.2093 sec\n",
      "Epoch 892, gen_loss: 0.4552, disc_loss: 0.0382\n",
      "Time for epoch 892 is 15.8535 sec\n",
      "Epoch 893, gen_loss: 0.4264, disc_loss: 0.0620\n",
      "Time for epoch 893 is 16.0705 sec\n",
      "Epoch 894, gen_loss: 0.4219, disc_loss: 0.0646\n",
      "Time for epoch 894 is 16.0029 sec\n",
      "Epoch 895, gen_loss: 0.4314, disc_loss: 0.0523\n",
      "Time for epoch 895 is 16.0391 sec\n",
      "Epoch 896, gen_loss: 0.4163, disc_loss: 0.0577\n",
      "Time for epoch 896 is 16.1285 sec\n",
      "Epoch 897, gen_loss: 0.4285, disc_loss: 0.0503\n",
      "Time for epoch 897 is 16.6937 sec\n",
      "Epoch 898, gen_loss: 0.4234, disc_loss: 0.0532\n",
      "Time for epoch 898 is 16.1258 sec\n",
      "Epoch 899, gen_loss: 0.4636, disc_loss: 0.0286\n",
      "Time for epoch 899 is 16.1069 sec\n",
      "Epoch 900, gen_loss: 0.4192, disc_loss: 0.0534\n",
      "Time for epoch 900 is 16.0963 sec\n",
      "Saved checkpoint for epoch 900: ./checkpoints/demo/ckpt-71\n",
      "Epoch 901, gen_loss: 0.4434, disc_loss: 0.0447\n",
      "Time for epoch 901 is 15.9941 sec\n",
      "Epoch 902, gen_loss: 0.4558, disc_loss: 0.0379\n",
      "Time for epoch 902 is 15.9361 sec\n",
      "Epoch 903, gen_loss: 0.4239, disc_loss: 0.0594\n",
      "Time for epoch 903 is 15.8048 sec\n",
      "Epoch 904, gen_loss: 0.4491, disc_loss: 0.0412\n",
      "Time for epoch 904 is 15.9777 sec\n",
      "Epoch 905, gen_loss: 0.4342, disc_loss: 0.0475\n",
      "Time for epoch 905 is 15.8543 sec\n",
      "Epoch 906, gen_loss: 0.4402, disc_loss: 0.0445\n",
      "Time for epoch 906 is 16.0589 sec\n",
      "Epoch 907, gen_loss: 0.4388, disc_loss: 0.0475\n",
      "Time for epoch 907 is 16.0313 sec\n",
      "Epoch 908, gen_loss: 0.4535, disc_loss: 0.0404\n",
      "Time for epoch 908 is 15.9135 sec\n",
      "Epoch 909, gen_loss: 0.4255, disc_loss: 0.0483\n",
      "Time for epoch 909 is 15.8960 sec\n",
      "Epoch 910, gen_loss: 0.4363, disc_loss: 0.0542\n",
      "Time for epoch 910 is 16.0361 sec\n",
      "Epoch 911, gen_loss: 0.4398, disc_loss: 0.0488\n",
      "Time for epoch 911 is 15.9949 sec\n",
      "Epoch 912, gen_loss: 0.4234, disc_loss: 0.0522\n",
      "Time for epoch 912 is 16.0175 sec\n",
      "Epoch 913, gen_loss: 0.4337, disc_loss: 0.0495\n",
      "Time for epoch 913 is 16.0089 sec\n",
      "Epoch 914, gen_loss: 0.4281, disc_loss: 0.0622\n",
      "Time for epoch 914 is 15.9428 sec\n",
      "Epoch 915, gen_loss: 0.4494, disc_loss: 0.0369\n",
      "Time for epoch 915 is 16.0923 sec\n",
      "Epoch 916, gen_loss: 0.4467, disc_loss: 0.0364\n",
      "Time for epoch 916 is 15.8707 sec\n",
      "Epoch 917, gen_loss: 0.4226, disc_loss: 0.0543\n",
      "Time for epoch 917 is 15.9551 sec\n",
      "Epoch 918, gen_loss: 0.4509, disc_loss: 0.0420\n",
      "Time for epoch 918 is 15.9869 sec\n",
      "Epoch 919, gen_loss: 0.4541, disc_loss: 0.0354\n",
      "Time for epoch 919 is 15.9669 sec\n",
      "Epoch 920, gen_loss: 0.4509, disc_loss: 0.0391\n",
      "Time for epoch 920 is 16.1148 sec\n",
      "Epoch 921, gen_loss: 0.4306, disc_loss: 0.0464\n",
      "Time for epoch 921 is 16.1181 sec\n",
      "Epoch 922, gen_loss: 0.4465, disc_loss: 0.0392\n",
      "Time for epoch 922 is 15.9114 sec\n",
      "Epoch 923, gen_loss: 0.4485, disc_loss: 0.0463\n",
      "Time for epoch 923 is 15.9146 sec\n",
      "Epoch 924, gen_loss: 0.4376, disc_loss: 0.0504\n",
      "Time for epoch 924 is 15.8921 sec\n",
      "Epoch 925, gen_loss: 0.4386, disc_loss: 0.0478\n",
      "Time for epoch 925 is 15.9415 sec\n",
      "Epoch 926, gen_loss: 0.4409, disc_loss: 0.0436\n",
      "Time for epoch 926 is 16.0141 sec\n",
      "Epoch 927, gen_loss: 0.4357, disc_loss: 0.0463\n",
      "Time for epoch 927 is 15.9384 sec\n",
      "Epoch 928, gen_loss: 0.4207, disc_loss: 0.0616\n",
      "Time for epoch 928 is 15.8998 sec\n",
      "Epoch 929, gen_loss: 0.4374, disc_loss: 0.0459\n",
      "Time for epoch 929 is 15.9844 sec\n",
      "Epoch 930, gen_loss: 0.4523, disc_loss: 0.0439\n",
      "Time for epoch 930 is 16.0420 sec\n",
      "Epoch 931, gen_loss: 0.4505, disc_loss: 0.0394\n",
      "Time for epoch 931 is 15.9122 sec\n",
      "Epoch 932, gen_loss: 0.4476, disc_loss: 0.0415\n",
      "Time for epoch 932 is 16.0038 sec\n",
      "Epoch 933, gen_loss: 0.4404, disc_loss: 0.0439\n",
      "Time for epoch 933 is 16.0977 sec\n",
      "Epoch 934, gen_loss: 0.4399, disc_loss: 0.0495\n",
      "Time for epoch 934 is 16.1244 sec\n",
      "Epoch 935, gen_loss: 0.4467, disc_loss: 0.0365\n",
      "Time for epoch 935 is 16.0869 sec\n",
      "Epoch 936, gen_loss: 0.4267, disc_loss: 0.0529\n",
      "Time for epoch 936 is 15.9404 sec\n",
      "Epoch 937, gen_loss: 0.4536, disc_loss: 0.0370\n",
      "Time for epoch 937 is 15.9423 sec\n",
      "Epoch 938, gen_loss: 0.4346, disc_loss: 0.0516\n",
      "Time for epoch 938 is 15.9395 sec\n",
      "Epoch 939, gen_loss: 0.4485, disc_loss: 0.0356\n",
      "Time for epoch 939 is 15.9546 sec\n",
      "Epoch 940, gen_loss: 0.4470, disc_loss: 0.0351\n",
      "Time for epoch 940 is 16.0065 sec\n",
      "Epoch 941, gen_loss: 0.4279, disc_loss: 0.0663\n",
      "Time for epoch 941 is 16.0337 sec\n",
      "Epoch 942, gen_loss: 0.4174, disc_loss: 0.0543\n",
      "Time for epoch 942 is 16.1396 sec\n",
      "Epoch 943, gen_loss: 0.4619, disc_loss: 0.0308\n",
      "Time for epoch 943 is 16.0152 sec\n",
      "Epoch 944, gen_loss: 0.4391, disc_loss: 0.0448\n",
      "Time for epoch 944 is 15.9645 sec\n",
      "Epoch 945, gen_loss: 0.4283, disc_loss: 0.0523\n",
      "Time for epoch 945 is 15.8817 sec\n",
      "Epoch 946, gen_loss: 0.4434, disc_loss: 0.0488\n",
      "Time for epoch 946 is 15.9494 sec\n",
      "Epoch 947, gen_loss: 0.4368, disc_loss: 0.0487\n",
      "Time for epoch 947 is 15.9385 sec\n",
      "Epoch 948, gen_loss: 0.4423, disc_loss: 0.0454\n",
      "Time for epoch 948 is 15.8930 sec\n",
      "Epoch 949, gen_loss: 0.4395, disc_loss: 0.0393\n",
      "Time for epoch 949 is 15.8748 sec\n",
      "Epoch 950, gen_loss: 0.4592, disc_loss: 0.0309\n",
      "Time for epoch 950 is 15.8778 sec\n",
      "Epoch 951, gen_loss: 0.4482, disc_loss: 0.0360\n",
      "Time for epoch 951 is 15.9089 sec\n",
      "Epoch 952, gen_loss: 0.4468, disc_loss: 0.0396\n",
      "Time for epoch 952 is 15.8688 sec\n",
      "Epoch 953, gen_loss: 0.4505, disc_loss: 0.0413\n",
      "Time for epoch 953 is 15.9006 sec\n",
      "Epoch 954, gen_loss: 0.4242, disc_loss: 0.0653\n",
      "Time for epoch 954 is 15.8997 sec\n",
      "Epoch 955, gen_loss: 0.4400, disc_loss: 0.0432\n",
      "Time for epoch 955 is 15.9226 sec\n",
      "Epoch 956, gen_loss: 0.4451, disc_loss: 0.0385\n",
      "Time for epoch 956 is 15.9920 sec\n",
      "Epoch 957, gen_loss: 0.4530, disc_loss: 0.0368\n",
      "Time for epoch 957 is 16.0388 sec\n",
      "Epoch 958, gen_loss: 0.4441, disc_loss: 0.0457\n",
      "Time for epoch 958 is 16.1376 sec\n",
      "Epoch 959, gen_loss: 0.4181, disc_loss: 0.0593\n",
      "Time for epoch 959 is 15.9183 sec\n",
      "Epoch 960, gen_loss: 0.4397, disc_loss: 0.0449\n",
      "Time for epoch 960 is 15.8231 sec\n",
      "Epoch 961, gen_loss: 0.4371, disc_loss: 0.0494\n",
      "Time for epoch 961 is 15.8951 sec\n",
      "Epoch 962, gen_loss: 0.4296, disc_loss: 0.0523\n",
      "Time for epoch 962 is 15.8918 sec\n",
      "Epoch 963, gen_loss: 0.4458, disc_loss: 0.0420\n",
      "Time for epoch 963 is 15.7789 sec\n",
      "Epoch 964, gen_loss: 0.4423, disc_loss: 0.0422\n",
      "Time for epoch 964 is 16.0806 sec\n",
      "Epoch 965, gen_loss: 0.4225, disc_loss: 0.0559\n",
      "Time for epoch 965 is 15.7167 sec\n",
      "Epoch 966, gen_loss: 0.4431, disc_loss: 0.0528\n",
      "Time for epoch 966 is 15.5677 sec\n",
      "Epoch 967, gen_loss: 0.4356, disc_loss: 0.0455\n",
      "Time for epoch 967 is 15.5682 sec\n",
      "Epoch 968, gen_loss: 0.4597, disc_loss: 0.0292\n",
      "Time for epoch 968 is 15.5677 sec\n",
      "Epoch 969, gen_loss: 0.4491, disc_loss: 0.0377\n",
      "Time for epoch 969 is 15.5607 sec\n",
      "Epoch 970, gen_loss: 0.4365, disc_loss: 0.0544\n",
      "Time for epoch 970 is 15.7080 sec\n",
      "Epoch 971, gen_loss: 0.4278, disc_loss: 0.0592\n",
      "Time for epoch 971 is 15.5780 sec\n",
      "Epoch 972, gen_loss: 0.4287, disc_loss: 0.0568\n",
      "Time for epoch 972 is 15.5590 sec\n",
      "Epoch 973, gen_loss: 0.4363, disc_loss: 0.0522\n",
      "Time for epoch 973 is 15.7010 sec\n",
      "Epoch 974, gen_loss: 0.4421, disc_loss: 0.0467\n",
      "Time for epoch 974 is 15.7428 sec\n",
      "Epoch 975, gen_loss: 0.4512, disc_loss: 0.0396\n",
      "Time for epoch 975 is 15.6783 sec\n",
      "Epoch 976, gen_loss: 0.4371, disc_loss: 0.0410\n",
      "Time for epoch 976 is 15.6879 sec\n",
      "Epoch 977, gen_loss: 0.4440, disc_loss: 0.0434\n",
      "Time for epoch 977 is 15.7308 sec\n",
      "Epoch 978, gen_loss: 0.4439, disc_loss: 0.0400\n",
      "Time for epoch 978 is 15.8376 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparas\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mN_EPOCH\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m      9\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image, caption \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[0;32m---> 12\u001b[0m     g_loss, d_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaption\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     g_total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m g_loss\n\u001b[1;32m     14\u001b[0m     d_total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m d_loss\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(dataset, hparas['N_EPOCH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "If you change anything during preprocessing of training dataset, you must make sure same operations have be done in testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_data_generator(caption, index):\n",
    "    caption = tf.cast(caption, tf.float32)\n",
    "    return caption, index\n",
    "\n",
    "def testing_dataset_generator(batch_size, data_generator):\n",
    "    data = pd.read_pickle('./dataset/testData.pkl')\n",
    "    captions = data['Captions'].values\n",
    "    caption = []\n",
    "    for i in range(len(captions)):\n",
    "        caption.append(captions[i])\n",
    "    caption = np.asarray(caption)\n",
    "    caption = caption.astype(np.int)\n",
    "    index = data['ID'].values\n",
    "    index = np.asarray(index)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((caption, index))\n",
    "    dataset = dataset.map(data_generator, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.repeat().batch(batch_size)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-af1cd83bac2f>:12: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  caption = caption.astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "testing_dataset = testing_dataset_generator(hparas['BATCH_SIZE'], testing_data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./dataset/testData.pkl')\n",
    "captions = data['Captions'].values\n",
    "\n",
    "NUM_TEST = len(captions)\n",
    "EPOCH_TEST = int(NUM_TEST / hparas['BATCH_SIZE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./inference/demo'):\n",
    "    os.makedirs('./inference/demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(dataset):\n",
    "    hidden = text_encoder.initialize_hidden_state()\n",
    "    sample_size = hparas['BATCH_SIZE']\n",
    "    sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "    \n",
    "    step = 0\n",
    "    start = time.time()\n",
    "    for captions, idx in dataset:\n",
    "        if step > EPOCH_TEST:\n",
    "            break\n",
    "        \n",
    "        fake_image = test_step(captions, sample_seed, hidden)\n",
    "        step += 1\n",
    "        for i in range(hparas['BATCH_SIZE']):\n",
    "            plt.imsave('./inference/demo/inference_{:04d}.jpg'.format(idx[i]), fake_image[i].numpy()*0.5 + 0.5)\n",
    "            \n",
    "    print('Time for inference is {:.4f} sec'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error when restoring from checkpoint or SavedModel at ./checkpoints/demo/ckpt-1: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./checkpoints/demo/ckpt-1\nPlease double-check that the path is correct. You may be missing the checkpoint suffix (e.g. the '-1' in 'path/to/ckpt-1').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/py_checkpoint_reader.py:92\u001b[0m, in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepattern\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# issue with throwing python exceptions from C++.\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./checkpoints/demo/ckpt-1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/tracking/util.py:2537\u001b[0m, in \u001b[0;36mCheckpoint.restore\u001b[0;34m(self, save_path, options)\u001b[0m\n\u001b[1;32m   2536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2537\u001b[0m   status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2538\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/tracking/util.py:2417\u001b[0m, in \u001b[0;36mCheckpoint.read\u001b[0;34m(self, save_path, options)\u001b[0m\n\u001b[1;32m   2416\u001b[0m options \u001b[38;5;241m=\u001b[39m options \u001b[38;5;129;01mor\u001b[39;00m checkpoint_options\u001b[38;5;241m.\u001b[39mCheckpointOptions()\n\u001b[0;32m-> 2417\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_saver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2418\u001b[0m metrics\u001b[38;5;241m.\u001b[39mAddCheckpointReadDuration(\n\u001b[1;32m   2419\u001b[0m     api_label\u001b[38;5;241m=\u001b[39m_CHECKPOINT_V2,\n\u001b[1;32m   2420\u001b[0m     microseconds\u001b[38;5;241m=\u001b[39m_get_duration_microseconds(start_time, time\u001b[38;5;241m.\u001b[39mtime()))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/tracking/util.py:1423\u001b[0m, in \u001b[0;36mTrackableSaver.restore\u001b[0;34m(self, save_path, options)\u001b[0m\n\u001b[1;32m   1421\u001b[0m   _ASYNC_CHECKPOINT_THREAD\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[0;32m-> 1423\u001b[0m reader \u001b[38;5;241m=\u001b[39m \u001b[43mpy_checkpoint_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNewCheckpointReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1424\u001b[0m graph_building \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/py_checkpoint_reader.py:96\u001b[0m, in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 96\u001b[0m   \u001b[43merror_translator\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/py_checkpoint_reader.py:31\u001b[0m, in \u001b[0;36merror_translator\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot found in checkpoint\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_message \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to find any \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatching files for\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m error_message:\n\u001b[0;32m---> 31\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors_impl\u001b[38;5;241m.\u001b[39mNotFoundError(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, error_message)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSliced checkpoints are not supported\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_message \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData type \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupported\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m error_message:\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./checkpoints/demo/ckpt-1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/ckpt-1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/tracking/util.py:2541\u001b[0m, in \u001b[0;36mCheckpoint.restore\u001b[0;34m(self, save_path, options)\u001b[0m\n\u001b[1;32m   2539\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()  \u001b[38;5;66;03m# Ensure restore operations have completed.\u001b[39;00m\n\u001b[1;32m   2540\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors_impl\u001b[38;5;241m.\u001b[39mNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 2541\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors_impl\u001b[38;5;241m.\u001b[39mNotFoundError(\n\u001b[1;32m   2542\u001b[0m       \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2543\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError when restoring from checkpoint or SavedModel at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2544\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00morig_save_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2545\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease double-check that the path is correct. You may be missing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2546\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe checkpoint suffix (e.g. the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-1\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath/to/ckpt-1\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2547\u001b[0m \u001b[38;5;66;03m# Create the save counter now so it gets initialized with other variables\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m \u001b[38;5;66;03m# when graph building. Creating it earlier would lead to errors when using,\u001b[39;00m\n\u001b[1;32m   2549\u001b[0m \u001b[38;5;66;03m# say, train.Saver() to save the model before initializing it.\u001b[39;00m\n\u001b[1;32m   2550\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_create_save_counter()\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Error when restoring from checkpoint or SavedModel at ./checkpoints/demo/ckpt-1: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./checkpoints/demo/ckpt-1\nPlease double-check that the path is correct. You may be missing the checkpoint suffix (e.g. the '-1' in 'path/to/ckpt-1')."
     ]
    }
   ],
   "source": [
    "checkpoint.restore(checkpoint_dir + '/ckpt-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for inference is 1.4534 sec\n"
     ]
    }
   ],
   "source": [
    "inference(testing_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception Score & Cosine Similarity\n",
    "- Similarity of images and the given contents. How similar are the generated images and the given texts?\n",
    "- KL divergence of generated images. Are the generated images very diverse?\n",
    "\n",
    "### Run evaluation script\n",
    "\n",
    "1. Open terminal and move to the folder containing inception_score.py. Otherwise you have to modify the path used in the file.\n",
    "2. Run python ./inception_score.py [argv1] [argv2] [argv3]\n",
    "    - argv1: directory of generated image (inference).\n",
    "    - argv2: directory of output file and its name.\n",
    "    - argv3: batch size. Please set batch size to 1, 2, 3, 7, 9, 21, 39 to avoid remainder.\n",
    "\n",
    "For exmaple, run following comment \n",
    "`python inception_score.py ../inference/demo ../score_demo.csv 39`\n",
    "\n",
    "It is better for you to know that evaluation needs to run on GPUs, please make sure the GPU resource is available."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
